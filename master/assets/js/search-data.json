{"0": {
    "doc": "CRDs",
    "title": "Custom Resources",
    "content": " ",
    "url": "/node-feature-discovery/master/usage/custom-resources.html#custom-resources",
    
    "relUrl": "/usage/custom-resources.html#custom-resources"
  },"1": {
    "doc": "CRDs",
    "title": "Table of contents",
    "content": ". | NodeFeature | NodeFeatureGroup | NodeFeatureRule | NodeResourceTopology | . NFD uses some Kubernetes custom resources. ",
    "url": "/node-feature-discovery/master/usage/custom-resources.html#table-of-contents",
    
    "relUrl": "/usage/custom-resources.html#table-of-contents"
  },"2": {
    "doc": "CRDs",
    "title": "NodeFeature",
    "content": "NodeFeature is an NFD-specific custom resource for communicating node features and node labeling requests. The nfd-master pod watches for NodeFeature objects, labels nodes as specified and uses the listed features as input when evaluating NodeFeatureRules. NodeFeature objects can be used for implementing 3rd party extensions (see customization guide for more details). apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeature metadata: labels: nfd.node.kubernetes.io/node-name: node-1 name: node-1-vendor-features spec: features: instances: vendor.device: elements: - attributes: model: \"xpu-1\" memory: \"4000\" type: \"fast\" - attributes: model: \"xpu-2\" memory: \"16000\" type: \"slow\" labels: vendor-xpu-present: \"true\" . ",
    "url": "/node-feature-discovery/master/usage/custom-resources.html#nodefeature",
    
    "relUrl": "/usage/custom-resources.html#nodefeature"
  },"3": {
    "doc": "CRDs",
    "title": "NodeFeatureGroup",
    "content": "NodeFeatureGroup is an NFD-specific custom resource that is designed for grouping nodes based on their features. NFD-Master watches for NodeFeatureGroup objects in the cluster and updates the status of the NodeFeatureGroup object with the list of nodes that match the feature group rules. The NodeFeatureGroup rules follow the same syntax as the NodeFeatureRule rules. apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeatureGroup metadata: name: node-feature-group-example spec: featureGroupRules: - name: \"node has kernel version discovered\" matchFeatures: - feature: kernel.version matchExpressions: major: {op: Exists} . NodeFeatureGroup API is an alpha feature and disabled by default in NFD version master. For more details and examples see the customization guide. ",
    "url": "/node-feature-discovery/master/usage/custom-resources.html#nodefeaturegroup",
    
    "relUrl": "/usage/custom-resources.html#nodefeaturegroup"
  },"4": {
    "doc": "CRDs",
    "title": "NodeFeatureRule",
    "content": "NodeFeatureRule is an NFD-specific custom resource that is designed for rule-based custom labeling of nodes. NFD-Master watches for NodeFeatureRule objects in the cluster and labels nodes according to the rules within. Some use cases are e.g. application specific labeling in a specific environments or being distributed by hardware vendors to create specific labels for their devices. apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeatureRule metadata: name: example-rule spec: rules: - name: \"example rule\" labels: \"example-custom-feature\": \"true\" # Label is created if all of the rules below match matchFeatures: # Match if \"veth\" kernel module is loaded - feature: kernel.loadedmodule matchExpressions: veth: {op: Exists} # Match if any PCI device with vendor 8086 exists in the system - feature: pci.device matchExpressions: vendor: {op: In, value: [\"8086\"]} . See the Customization guide for full documentation of the NodeFeatureRule resource and its usage. The deployment/nodefeaturerule/samples/ directory contains sample NodeFeatureRule objects that replicate the built-in default feature labels generated by NFD. The sample rules can be used as a base to customize NFD feature labels. To use them in place of the the NFD built-in labels, the corresponding feature source(s) of nfd-worker should be disabled with the core.labelSources configuration option. ",
    "url": "/node-feature-discovery/master/usage/custom-resources.html#nodefeaturerule",
    
    "relUrl": "/usage/custom-resources.html#nodefeaturerule"
  },"5": {
    "doc": "CRDs",
    "title": "NodeResourceTopology",
    "content": "When run with NFD-Topology-Updater, NFD creates NodeResourceTopology objects corresponding to node resource hardware topology such as: . apiVersion: topology.node.k8s.io/v1alpha1 kind: NodeResourceTopology metadata: name: node1 topologyPolicies: [\"SingleNUMANodeContainerLevel\"] zones: - name: node-0 type: Node resources: - name: cpu capacity: 20 allocatable: 16 available: 10 - name: vendor/nic1 capacity: 3 allocatable: 3 available: 3 - name: node-1 type: Node resources: - name: cpu capacity: 30 allocatable: 30 available: 15 - name: vendor/nic2 capacity: 6 allocatable: 6 available: 6 - name: node-2 type: Node resources: - name: cpu capacity: 30 allocatable: 30 available: 15 - name: vendor/nic1 capacity: 3 allocatable: 3 available: 3 . The NodeResourceTopology objects created by NFD can be used to gain insight into the allocatable resources along with the granularity of those resources at a per-zone level (represented by node-0 and node-1 in the above example) or can be used by an external entity (e.g. topology-aware scheduler plugin) to take an action based on the gathered information. ",
    "url": "/node-feature-discovery/master/usage/custom-resources.html#noderesourcetopology",
    
    "relUrl": "/usage/custom-resources.html#noderesourcetopology"
  },"6": {
    "doc": "CRDs",
    "title": "CRDs",
    "content": " ",
    "url": "/node-feature-discovery/master/usage/custom-resources.html",
    
    "relUrl": "/usage/custom-resources.html"
  },"7": {
    "doc": "Customization guide",
    "title": "Customization guide",
    "content": " ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html",
    
    "relUrl": "/usage/customization-guide.html"
  },"8": {
    "doc": "Customization guide",
    "title": "Table of contents",
    "content": ". | Overview | NodeFeature custom resource . | A NodeFeature example | Feature types | . | NodeFeatureRule custom resource . | A NodeFeatureRule example | Node tainting | . | NodeFeatureGroup custom resource . | A NodeFeatureGroup example | . | Local feature source . | An example | Feature files | Input format | Mounts | . | Custom feature source . | An example custom feature source configuration | Additional configuration directory | . | Node labels | Feature rule format . | Fields | Available features | Templating | Backreferences | Examples | . | . ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#table-of-contents",
    
    "relUrl": "/usage/customization-guide.html#table-of-contents"
  },"9": {
    "doc": "Customization guide",
    "title": "Overview",
    "content": "NFD provides multiple extension points for vendor and application specific labeling: . | NodeFeature objects can be used to communicate “raw” node features and node labeling requests to nfd-master. | NodeFeatureRule objects provide a way to deploy custom labeling rules via the Kubernetes API. | local feature source of nfd-worker creates labels by reading text files. | custom feature source of nfd-worker creates labels based on user-specified rules. | . ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#overview",
    
    "relUrl": "/usage/customization-guide.html#overview"
  },"10": {
    "doc": "Customization guide",
    "title": "NodeFeature custom resource",
    "content": "NodeFeature objects provide a way for 3rd party extensions to advertise custom features, both as “raw” features that serve as input to NodeFeatureRule objects and as feature labels directly. Note that RBAC rules must be created for each extension for them to be able to create and manipulate NodeFeature objects in their namespace. A NodeFeature example . Consider the following referential example: . apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeature metadata: labels: nfd.node.kubernetes.io/node-name: node-1 name: vendor-features-for-node-1 spec: # Features for NodeFeatureRule matching features: flags: vendor.flags: elements: feature-x: {} feature-y: {} attributes: vendor.config: elements: setting-a: \"auto\" knob-b: \"123\" instances: vendor.devices: elements: - attributes: model: \"dev-1000\" vendor: \"acme\" - attributes: model: \"dev-2000\" vendor: \"acme\" # Labels to be created labels: vendor.io/feature.enabled: \"true\" . The object targets node named node-1. It lists two “flag type” features under the vendor.flags domain, two “attribute type” features and under the vendor.config domain and two “instance type” features under the vendor.devices domain. These features will not be directly affecting the node labels but they will be used as input when the NodeFeatureRule objects are evaluated. In addition, the example requests directly the vendor.io/feature.enabled=true node label to be created. The nfd.node.kubernetes.io/node-name=&lt;node-name&gt; must be in place for each NodeFeature object as NFD uses it to determine the node which it is targeting. Feature types . Features have three different types: . | flag features: a set of names without any associated values, e.g. CPUID flags or loaded kernel modules | attribute features: a set of names each of which has a single value associated with it (essentially a map of key-value pairs), e.g. kernel config flags or os release information | instance features: a list of instances, each of which has multiple attributes (key-value pairs of their own) associated with it, e.g. PCI or USB devices | . ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#nodefeature-custom-resource",
    
    "relUrl": "/usage/customization-guide.html#nodefeature-custom-resource"
  },"11": {
    "doc": "Customization guide",
    "title": "NodeFeatureRule custom resource",
    "content": "NodeFeatureRule objects provide an easy way to create vendor or application specific labels and taints. It uses a flexible rule-based mechanism for creating labels and optionally taints based on node features. A NodeFeatureRule example . Consider the following referential example: . apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeatureRule metadata: name: my-sample-rule-object spec: rules: - name: \"my sample rule\" labels: \"feature.node.kubernetes.io/my-sample-feature\": \"true\" matchFeatures: - feature: kernel.loadedmodule matchExpressions: dummy: {op: Exists} - feature: kernel.config matchExpressions: X86: {op: In, value: [\"y\"]} . It specifies one rule which creates node label feature.node.kubernetes.io/my-sample-feature=true if both of the following conditions are true (matchFeatures implements a logical AND over the matchers): . | The dummy network driver module has been loaded | X86 option in kernel config is set to =y | . Create a NodeFeatureRule with a yaml file: . kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/node-feature-discovery/master/examples/nodefeaturerule.yaml . Now, on X86 platforms the feature label appears after doing modprobe dummy on a system and correspondingly the label is removed after rmmod dummy. Note a re-labeling delay up to the sleep-interval of nfd-worker (1 minute by default). See Feature rule format for detailed description of available fields and how to write labeling rules. Node tainting . In some circumstances, it is desirable to keep nodes with specialized hardware away from running general workload and instead leave them for workloads that need the specialized hardware. One way to achieve it is to taint the nodes with the specialized hardware and add corresponding toleration to pods that require the special hardware. NFD offers node tainting functionality which is disabled by default. User can define one or more custom taints via the taints field of the NodeFeatureRule CR. The same rule-based mechanism is applied here and the NFD taints only rule matching nodes. To enable the tainting feature, --enable-taints flag needs to be set to true. If the flag --enable-taints is set to false (i.e. disabled), taints defined in the NodeFeatureRule CR have no effect and will be ignored by the NFD master. See documentation of the taints field for detailed description how to specify taints in the NodeFeatureRule object. NOTE: Before enabling any taints, make sure to edit nfd-worker daemonset to tolerate the taints to be created. Otherwise, already running pods that do not tolerate the taint are evicted immediately from the node including the nfd-worker pod. ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#nodefeaturerule-custom-resource",
    
    "relUrl": "/usage/customization-guide.html#nodefeaturerule-custom-resource"
  },"12": {
    "doc": "Customization guide",
    "title": "NodeFeatureGroup custom resource",
    "content": "NodeFeatureGroup API is an alpha feature and disabled by default in NFD version master. Use the NodeFeatureAPI feature gate to enable it. NodeFeatureGroup objects provide a way to create node groups that share the same set of features. The NodeFeatureGroup object spec consists of a list of NodeFeatureRule that follow the same format as the NodeFeatureRule, but the difference in this case is that nodes that match any of the rules in the NodeFeatureGroup will be listed in the NodeFeatureGroup status. A NodeFeatureGroup example . Consider the following referential example: . apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeatureGroup metadata: name: node-feature-group-example spec: featureGroupRules: - name: \"kernel version\" matchFeatures: - feature: kernel.version matchExpressions: major: {op: In, value: [\"6\"]} status: nodes: - name: node-1 - name: node-2 - name: node-3 . The object specifies a group of nodes that share the same kernel.version.major (Linux kernel v6.x). Create a NodeFeatureGroup with a yaml file: . kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/node-feature-discovery/master/examples/nodefeaturegroup.yaml . See Feature rule format for detailed description of available fields and how to write group filtering rules. ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#nodefeaturegroup-custom-resource",
    
    "relUrl": "/usage/customization-guide.html#nodefeaturegroup-custom-resource"
  },"13": {
    "doc": "Customization guide",
    "title": "Local feature source",
    "content": "NFD-Worker has a special feature source named local which is an integration point for external feature detectors. It provides a mechanism for pluggable extensions, allowing the creation of new user-specific features and even overriding built-in labels. The local feature source uses feature files. The features discovered by the local source can further be used in label rules specified in NodeFeatureRule objects and the custom feature source. NOTE: Be careful when creating and/or updating feature files while NFD is running. To avoid race conditions you should write into a temporary file, and atomically create/update the original file by doing a file rename operation. NFD ignores dot files, so temporary file can be written to the same directory and renamed (.my.feature -&gt; my.feature) once file is complete. Both file names should (obviously) be unique for the given application. An example . Consider a plaintext file /etc/kubernetes/node-feature-discovery/features.d/my-features having the following contents: . feature.node.kubernetes.io/my-feature.1 feature.node.kubernetes.io/my-feature.2=myvalue vendor.io/my-feature.3=456 . This will translate into the following node labels: . feature.node.kubernetes.io/my-feature.1: \"true\" feature.node.kubernetes.io/my-feature.2: \"myvalue\" vendor.io/my-feature.3: \"456\" . Feature files . The local source reads files found in /etc/kubernetes/node-feature-discovery/features.d/. File content is parsed and translated into node labels, see the input format below. Input format . The feature files are expected to contain features in simple key-value pairs, separated by newlines: . # This is a comment &lt;key&gt;[=&lt;value&gt;] . The label value defaults to true, if not specified. Label namespace must be specified with &lt;namespace&gt;/&lt;name&gt;[=&lt;value&gt;]. NOTE: The feature file size limit it 64kB. The feature file will be ignored if the size limit is exceeded. Comment lines (starting with #) are ignored. Adding following line anywhere to feature file defines date when its content expires / is ignored: . # +expiry-time=2023-07-29T11:22:33Z . Also, the expiry-time value would stay the same during the processing of the feature file until another expiry-time directive is encountered. Considering the following file: . # +expiry-time=2012-07-28T11:22:33Z vendor.io/feature1=featureValue # +expiry-time=2080-07-28T11:22:33Z vendor.io/feature2=featureValue2 # +expiry-time=2070-07-28T11:22:33Z vendor.io/feature3=featureValue3 # +expiry-time=2002-07-28T11:22:33Z vendor.io/feature4=featureValue4 . After processing the above file, only vendor.io/feature2 and vendor.io/feature3 would be included in the list of accepted features. NOTE: The time format supported is RFC3339. Also, the expiry-time tag is only evaluated in each re-discovery period, and the expiration of node labels is not tracked. To exclude specific features from the local.feature Feature, you can use the # +no-feature directive. The # +no-label directive causes the feature to be excluded from the local.label Feature and a node label not to be generated. Considering the following file: . # +no-feature vendor.io/label-only=value vendor.io/my-feature=value vendor.io/foo=bar # +no-label foo=baz . Processing the above file would result in the following Features: . local.features: foo: baz vendor.io/my-feature: value local.labels: vendor.io/label-only: value vendor.io/my-feature: value . and the following labels added to the Node: . vendor.io/label-only=value vendor.io/my-feature=value . NOTE: use of unprefixed label names (like foo=bar) should not be used. In NFD master unprefixed names will be automatically prefixed with feature.node.kubernetes.io/ but this will change in a future version (see the DisableAutoPrefix feature gate). Unprefixed names for plain Features (tagged with #+no-label) can be used without restrictions, however. Mounts . The standard NFD deployments contain hostPath mounts for /etc/kubernetes/node-feature-discovery/features.d/, making these directories from the host available inside the nfd-worker container. Injecting labels from other pods . One use case for the feature files is detecting features in other Pods outside NFD, e.g. in Kubernetes device plugins. By using the same hostPath mounts /etc/kubernetes/node-feature-discovery/features.d/ in the side-car (e.g. device plugin) creates a shared area for deploying feature files to NFD. ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#local-feature-source",
    
    "relUrl": "/usage/customization-guide.html#local-feature-source"
  },"14": {
    "doc": "Customization guide",
    "title": "Custom feature source",
    "content": "The custom feature source in nfd-worker provides a rule-based mechanism for label creation, similar to the NodeFeatureRule objects. The difference is that the rules are specified in the worker configuration instead of a Kubernetes API object. See worker configuration for instructions how to set-up and manage the worker configuration. An example custom feature source configuration . Consider the following referential configuration for nfd-worker: . core: labelSources: [\"custom\"] sources: custom: - name: \"my sample rule\" labels: \"feature.node.kubenernetes.io/my-sample-feature\": \"true\" matchFeatures: - feature: kernel.loadedmodule matchExpressions: dummy: {op: Exists} - feature: kernel.config matchExpressions: X86: {op: In, value: [\"y\"]} . It specifies one rule which creates node label feature.node.kubenernetes.io/my-sample-feature=true if both of the following conditions are true (matchFeatures implements a logical AND over the matchers): . | The dummy network driver module has been loaded | X86 option in kernel config is set to =y | . In addition, the configuration only enables the custom source, disabling all built-in labels. Now, on X86 platforms the feature label appears after doing modprobe dummy on a system and correspondingly the label is removed after rmmod dummy. Note a re-labeling delay up to the sleep-interval of nfd-worker (1 minute by default). Additional configuration directory . In addition to the rules defined in the nfd-worker configuration file, the custom feature source can read more configuration files located in the /etc/kubernetes/node-feature-discovery/custom.d/ directory. This makes more dynamic and flexible configuration easier. As an example, consider having file /etc/kubernetes/node-feature-discovery/custom.d/my-rule.yaml with the following content: . - name: \"my e1000 rule\" labels: \"feature.node.kubenernetes.io/e1000.present\": \"true\" matchFeatures: - feature: kernel.loadedmodule matchExpressions: e1000: {op: Exists} . This simple rule will create feature.node.kubenernetes.io/e1000.present=true label if the e1000 kernel module has been loaded. The samples/custom-rules kustomize overlay sample contains an example for deploying a custom rule from a ConfigMap. ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#custom-feature-source",
    
    "relUrl": "/usage/customization-guide.html#custom-feature-source"
  },"15": {
    "doc": "Customization guide",
    "title": "Node labels",
    "content": "Feature labels have the following format: . &lt;namespace&gt;/&lt;name&gt; = &lt;value&gt; . The namespace part (i.e. prefix) of the labels is controlled by nfd: . | All built-in labels use feature.node.kubernetes.io. | Namespaces may be excluded with the -deny-label-ns command line flag of nfd-master . | To allow specific namespaces that were denied, you can use -extra-label-ns command line flag of nfd-master. e.g: nfd-master -deny-label-ns=\"*\" -extra-label-ns=example.com | . | . ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#node-labels",
    
    "relUrl": "/usage/customization-guide.html#node-labels"
  },"16": {
    "doc": "Customization guide",
    "title": "Feature rule format",
    "content": "This section describes the rule format used in NodeFeatureRule objects and in the configuration of the custom feature source. It is based on a generic feature matcher that covers all features discovered by nfd-worker. The rules rely on a unified data model of the available features and a generic expression-based format. Features that can be used in the rules are described in detail in available features below. Take this rule as a referential example: . - name: \"my feature rule\" labels: \"feature.node.kubernetes.io/my-special-feature\": \"my-value\" matchFeatures: - feature: cpu.cpuid matchExpressions: AVX512F: {op: Exists} - feature: kernel.version matchExpressions: major: {op: In, value: [\"5\"]} minor: {op: Gt, value: [\"1\"]} - feature: pci.device matchExpressions: vendor: {op: In, value: [\"8086\"]} class: {op: In, value: [\"0200\"]} . This will yield feature.node.kubernetes.io/my-special-feature=my-value node label if all of these are true (matchFeatures implements a logical AND over the matchers): . | the CPU has AVX512F capability | kernel version is 5.2 or later (must be v5.x) | an Intel network controller is present | . Fields . name . The .name field is required and used as an identifier of the rule. labels . The .labels is a map of the node labels to create if the rule matches. Take this rule as a referential example: . apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeatureRule metadata: name: my-sample-rule-object spec: rules: - name: \"my dynamic label value rule\" labels: feature.node.kubernetes.io/linux-lsm-enabled: \"@kernel.config.LSM\" feature.node.kubernetes.io/custom-label: \"customlabel\" . Label linux-lsm-enabled uses the @ notation for dynamic values. The value of the label will be the value of the attribute LSM of the feature kernel.config. The @&lt;feature-name&gt;.&lt;element-name&gt; format can be used to inject values of detected features to the label. See available features for possible values to use. This will yield into the following node label: . labels: ... feature.node.kubernetes.io/linux-lsm-enabled: apparmor feature.node.kubernetes.io/custom-label: \"customlabel\" . labelsTemplate . The .labelsTemplate field specifies a text template for dynamically creating labels based on the matched features. See templating for details. NOTE: The labels field has priority over labelsTemplate, i.e. labels specified in the labels field will override anything originating from labelsTemplate. annotations . The .annotations field is a list of features to be advertised as node annotations. Take this rule as a referential example: . apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeatureRule metadata: name: feature-annotations-example spec: rules: - name: \"annotation-example\" annotations: feature.node.kubernetes.io/defaul-ns-annotation: \"foo\" custom.vendor.io/feature: \"baz\" matchFeatures: - feature: kernel.version matchExpressions: major: {op: Exists} . This will yield into the following node annotations: . annotations: ... feature.node.kubernetes.io/defaul-ns-annotation: \"foo\" custom.vendor.io/feature: \"baz\" ... NFD enforces some limitations to the namespace (or prefix)/ of the annotations: . | kubernetes.io/ and its sub-namespaces (like sub.ns.kubernetes.io/) cannot generally be used | the only exception is feature.node.kubernetes.io/ and its sub-namespaces (like sub.ns.feature.node.kubernetes.io) | unprefixed names (like my-annotation) should not be used. In NFD master unprefixed names will be automatically prefixed with feature.node.kubernetes.io/ but this will change in a future version (see the DisableAutoPrefix feature gate). | . NOTE: The annotations field has will only advertise features via node annotations the features won’t be advertised as node labels unless they are specified in the labels field. taints . taints is a list of taint entries and each entry can have key, value and effect, where the value is optional. Effect could be NoSchedule, PreferNoSchedule or NoExecute. To learn more about the meaning of these effects, check out k8s documentation. Example NodeFeatureRule with taints: . apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeatureRule metadata: name: my-sample-rule-object spec: rules: - name: \"my sample taint rule\" taints: - effect: PreferNoSchedule key: \"feature.node.kubernetes.io/special-node\" value: \"true\" - effect: NoExecute key: \"feature.node.kubernetes.io/dedicated-node\" matchFeatures: - feature: kernel.loadedmodule matchExpressions: dummy: {op: Exists} - feature: kernel.config matchExpressions: X86: {op: In, value: [\"y\"]} . In this example, if the my sample taint rule rule is matched, feature.node.kubernetes.io/pci-0300_1d0f.present=true:NoExecute and feature.node.kubernetes.io/cpu-cpuid.ADX:NoExecute taints are set on the node. There are some limitations to the namespace part (i.e. prefix/) of the taint key: . | kubernetes.io/ and its sub-namespaces (like sub.ns.kubernetes.io/) cannot generally be used | the only exception is feature.node.kubernetes.io/ and its sub-namespaces (like sub.ns.feature.node.kubernetes.io) | unprefixed keys (like foo) keys are disallowed | . NOTE: taints field is not available for the custom rules of nfd-worker and only for NodeFeatureRule objects. vars . The .vars field is a map of values (key-value pairs) to store for subsequent rules to use. In other words, these are variables that are not advertised as node labels. See backreferences for more details on the usage of vars. extendedResources . The .extendedResources field is a list of extended resources to advertise. See extended resources for more details. Take this rule as a referential example: . apiVersion: nfd.k8s-sigs.io/v1alpha1 kind: NodeFeatureRule metadata: name: my-extended-resource-rule spec: rules: - name: \"my extended resource rule\" extendedResources: vendor.io/dynamic: \"@kernel.version.major\" vendor.io/static: \"123\" matchFeatures: - feature: kernel.version matchExpressions: major: {op: Exists} . The extended resource vendor.io/dynamic is defined in the form @feature.attribute. The value of the extended resource will be the value of the attribute major of the feature kernel.version. The @&lt;feature-name&gt;.&lt;element-name&gt; format can be used to inject values of detected features to the extended resource. See available features for possible values to use. Note that the value must be eligible as a Kubernetes resource quantity. This will yield into the following node status: . allocatable: ... vendor.io/dynamic: \"5\" vendor.io/static: \"123\" ... capacity: ... vendor.io/dynamic: \"5\" vendor.io/static: \"123\" ... There are some limitations to the namespace part (i.e. prefix)/ of the Extended Resources names: . | kubernetes.io/ and its sub-namespaces (like sub.ns.kubernetes.io/) cannot generally be used | the only exception is feature.node.kubernetes.io/ and its sub-namespaces (like sub.ns.feature.node.kubernetes.io) | unprefixed names (like my-er) site.version }} unprefixed names will be automatically prefixed with feature.node.kubernetes.io/ but this will change in a future version (see the DisableAutoPrefix feature gate). | . NOTE: .extendedResources is not supported by the custom feature source – it can only be used in NodeFeatureRule objects. varsTemplate . The .varsTemplate field specifies a text template for dynamically creating vars based on the matched features. See templating for details on using templates and backreferences for more details on the usage of vars. NOTE: The vars field has priority over varsTemplate, i.e. vars specified in the vars field will override anything originating from varsTemplate. matchFeatures . The .matchFeatures field specifies a feature matcher, consisting of a list of feature matcher terms. It implements a logical AND over the terms i.e. all of them must match for the rule to trigger. matchFeatures: - feature: &lt;feature-name&gt; matchExpressions: &lt;key&gt;: op: &lt;op&gt; value: - &lt;value-1&gt; - ... matchName: op: &lt;op&gt; value: - &lt;value-1&gt; - ... The .matchFeatures[].feature field specifies the feature which to evaluate. NOTE:If both matchExpressions and matchName are specified, they both must match. matchExpressions . The .matchFeatures[].matchExpressions field is used to match against the value(s) of a feature. The matchExpressions field consists of a set of expressions, each of which is evaluated against all elements of the specified feature. matchExpressions: &lt;key&gt;: op: &lt;op&gt; value: - &lt;value-1&gt; - ... type: &lt;type&gt; . In each MatchExpression the key specifies the name of of the feature element (flag and attribute features) or name of the attribute (instance features) which to look for. The behavior of MatchExpression depends on the feature type: . | for flag and attribute features the MatchExpression operates on the feature element whose name matches the &lt;key&gt; | for instance features all MatchExpressions are evaluated against the attributes of each instance separately | . The op field specifies the operator to apply. Valid values are described below. | Operator | Number of values | Matches when | . | In | 1 or greater | Input is equal to one of the values | . | NotIn | 1 or greater | Input is not equal to any of the values | . | InRegexp | 1 or greater | Values of the MatchExpression are treated as regexps and input matches one or more of them | . | Exists | 0 | The key exists | . | DoesNotExist | 0 | The key does not exists | . | Gt | 1 | Input is greater than the value. Both the input and value must be integer numbers. | . | Ge | 1 | Input is greater than or equal to the value. Both the input and value must be integer numbers. | . | Lt | 1 | Input is less than the value. Both the input and value must be integer numbers. | . | Le | 1 | Input is less than or equal to the value. Both the input and value must be integer numbers. | . | GtLt | 2 | Input is between two values. Both the input and value must be integer numbers. | . | GeLe | 2 | Input falls within a range that includes the boundary values. Both the input and value must be integer numbers. | . | IsTrue | 0 | Input is equal to “true” | . | IsFalse | 0 | Input is equal “false” | . The value field of MatchExpression is a list of string arguments to the operator. Type optional type field specifies the type of the value field. Valid types for specific operators are described below. | Type | Description | Supported Operators | . | version | Input is recognized as a version in the following formats (major.minor.patch) %d.%d.%d, %d.%d, %d (e.g., “1.2.3”, “1.2”, “1”) | Gt,Ge,Lt,Le,GtLt,GeLe | . matchName . The .matchFeatures[].matchName field is used to match against the name(s) of a feature (whereas the matchExpressions field matches against the value(s). The matchName field consists of a single expression which is evaulated against the name of each element of the specified feature. matchName: op: &lt;op&gt; value: - &lt;value-1&gt; - ... The behavior of matchName depends on the feature type: . | for flag and attribute features the expression is evaluated against the name of each element | for instance features the expression is evaluated against the name of each attribute, for each element (instance) separately (matches if the attributes of any of the elements satisfy the expression) | . The op field specifies the operator to apply. Same operators as for matchExpressions above are available. | Operator | Number of values | Matches | . | In | 1 or greater | All name is equal to one of the values | . | NotIn | 1 or greater | All name that is not equal to any of the values | . | InRegexp | 1 or greater | All name that matches any of the values (treated as regexps) | . | Exists | 0 | All elements | . Other operators are not practical with matchName (DoesNotExist never matches; Gt,Lt and GtLt are only usable if feature names are integers; IsTrue and IsFalse are only usable if the feature name is true or false). The value field is a list of string arguments to the operator. An example: . matchFeatures: - feature: cpu.cpuid matchName: {op: InRegexp, value: [\"^AVX\"]} . The snippet above would match if any CPUID feature starting with AVX is present (e.g. AVX1 or AVX2 or AVX512F etc). matchAny . The .matchAny field is a list of of matchFeatures matchers. A logical OR is applied over the matchers, i.e. at least one of them must match for the rule to trigger. Consider the following example: . matchAny: - matchFeatures: - feature: kernel.loadedmodule matchExpressions: kmod-1: {op: Exists} - feature: pci.device matchExpressions: vendor: {op: In, value: [\"0eee\"]} class: {op: In, value: [\"0200\"]} - matchFeatures: - feature: kernel.loadedmodule matchExpressions: kmod-2: {op: Exists} - feature: pci.device matchExpressions: vendor: {op: In, value: [\"0fff\"]} class: {op: In, value: [\"0200\"]} . This matches if kernel module kmod-1 is loaded and a network controller from vendor 0eee is present, OR, if kernel module kmod-2 has been loaded and a network controller from vendor 0fff is present (OR both of these conditions are true). Available features . The following features are available for matching: . | Feature | Feature types | Elements | Value type | Description | . | cpu.cpuid | flag |   |   | Supported CPU capabilities | . |   |   | &lt;cpuid-flag&gt; |   | CPUID flag is present | . |   | attribute |   |   | CPU capability attributes | . |   |   | AVX10_VERSION | int | AVX10 vector ISA version (if supported) | . | cpu.cstate | attribute |   |   | Status of cstates in the intel_idle cpuidle driver | . |   |   | enabled | bool | ‘true’ if cstates are set, otherwise ‘false’. Does not exist of intel_idle driver is not active. | . | cpu.model | attribute |   |   | CPU model related attributes | . |   |   | family | int | CPU family | . |   |   | vendor_id | string | CPU vendor ID | . |   |   | id | int | CPU model ID | . |   |   | hypervisor | string | Hypervisor type information from /proc/sysinfo (s390x-only feature) | . | cpu.pstate | attribute |   |   | State of the Intel pstate driver. Does not exist if the driver is not enabled. | . |   |   | status | string | Status of the driver, possible values are ‘active’ and ‘passive’ | . |   |   | turbo | bool | ‘true’ if turbo frequencies are enabled, otherwise ‘false’ | . |   |   | scaling | string | Active scaling_governor, possible values are ‘powersave’ or ‘performance’. | . | cpu.rdt | attribute |   |   | Intel RDT capabilities supported by the system | . |   |   | &lt;rdt-flag&gt; |   | RDT capability is supported, see RDT flags for details | . |   |   | RDTL3CA_NUM_CLOSID | int | The number or available CLOSID (Class of service ID) for Intel L3 Cache Allocation Technology | . | cpu.security | attribute |   |   | Features related to security and trusted execution environments | . |   |   | sgx.enabled | bool | true if Intel SGX (Software Guard Extensions) has been enabled, otherwise does not exist | . |   |   | sgx.epc | int | The total amount Intel SGX Encrypted Page Cache memory in bytes. It’s only present if sgx.enabled is true. | . |   |   | se.enabled | bool | true if IBM Secure Execution for Linux is available and has been enabled, otherwise does not exist | . |   |   | tdx.enabled | bool | true if Intel TDX (Trusted Domain Extensions) is available on the host and has been enabled, otherwise does not exist | . |   |   | tdx.total_keys | int | The total amount of keys an Intel TDX (Trusted Domain Extensions) host can provide. It’s only present if tdx.enabled is true. | . |   |   | tdx.protected | bool | true if a guest VM was started using Intel TDX (Trusted Domain Extensions), otherwise does not exist. | . |   |   | sev.enabled | bool | true if AMD SEV (Secure Encrypted Virtualization) is available on the host and has been enabled, otherwise does not exist | . |   |   | sev.es.enabled | bool | true if AMD SEV-ES (Encrypted State supported) is available on the host and has been enabled, otherwise does not exist | . |   |   | sev.snp.enabled | bool | true if AMD SEV-SNP (Secure Nested Paging supported) is available on the host and has been enabled, otherwise does not exist | . |   |   | sev.asids | int | The total amount of AMD SEV address-space identifiers (ASIDs), based on the /sys/fs/cgroup/misc.capacity information. | . |   |   | sev.encrypted_state_ids | int | The total amount of AMD SEV-ES and SEV-SNP supported, based on the /sys/fs/cgroup/misc.capacity information. | . | cpu.sst | attribute |   |   | Intel SST (Speed Select Technology) capabilities | . |   |   | bf.enabled | bool | true if Intel SST-BF (Intel Speed Select Technology - Base frequency) has been enabled, otherwise does not exist | . | cpu.topology | attribute |   |   | CPU topology related features | . |   |   | hardware_multithreading | bool | Hardware multithreading, such as Intel HTT, is enabled | . |   |   | socket_count | int | Number of CPU Sockets | . | cpu.coprocessor | attribute |   |   | CPU Coprocessor related features | . |   |   | nx_gzip | bool | Nest Accelerator GZIP support is enabled | . | kernel.config | attribute |   |   | Kernel configuration options | . |   |   | &lt;config-flag&gt; | string | Value of the kconfig option | . | kernel.loadedmodule | flag |   |   | Kernel modules loaded on the node as reported by /proc/modules | . | kernel.enabledmodule | flag |   |   | Kernel modules loaded on the node and available as built-ins as reported by modules.builtin | . |   |   | mod-name |   | Kernel module &lt;mod-name&gt; is loaded | . | kernel.selinux | attribute |   |   | Kernel SELinux related features | . |   |   | enabled | bool | true if SELinux has been enabled and is in enforcing mode, otherwise false | . | kernel.version | attribute |   |   | Kernel version information | . |   |   | full | string | Full kernel version (e.g. ‘4.5.6-7-g123abcde’) | . |   |   | major | int | First component of the kernel version (e.g. ‘4’) | . |   |   | minor | int | Second component of the kernel version (e.g. ‘5’) | . |   |   | revision | int | Third component of the kernel version (e.g. ‘6’) | . | local.label | attribute |   |   | Labels from feature files, i.e. labels from the local feature source | . | local.feature | attribute |   |   | Features from feature files, i.e. features from the local feature source | . |   |   | &lt;label-name&gt; | string | Label &lt;label-name&gt; created by the local feature source, value equals the value of the label | . | memory.nv | instance |   |   | NVDIMM devices present in the system | . |   |   | &lt;sysfs-attribute&gt; | string | Value of the sysfs device attribute, available attributes: devtype, mode | . | memory.numa | attribute |   |   | NUMA nodes | . |   |   | is_numa | bool | true if NUMA architecture, false otherwise | . |   |   | node_count | int | Number of NUMA nodes | . | memory.swap | attribute |   |   | Swap enabled on node | . |   |   | enabled | bool | true if swap partition detected, false otherwise | . | memory.hugepages | attribute |   |   | Discovery of supported huge pages size on node | . |   |   | enabled | bool | true if total number of huge pages (of any page size) have been configured, otherwise false | . |   |   | hugepages-&lt;page-size&gt; | string | Total number of huge pages (e.g., hugepages-1Gi=16) | . | network.device | instance |   |   | Physical (non-virtual) network interfaces present in the system | . |   |   | name | string | Name of the network interface | . |   |   | &lt;sysfs-attribute&gt; | string | Sysfs network interface attribute, available attributes: operstate, speed, sriov_numvfs, sriov_totalvfs, mtu | . | network.virtual | instance |   |   | Virtual network interfaces present in the system | . |   |   | name | string | Name of the network interface | . |   |   | &lt;sysfs-attribute&gt; | string | Sysfs network interface attribute, available attributes: operstate, speed, mtu | . | pci.device | instance |   |   | PCI devices present in the system | . |   |   | &lt;sysfs-attribute&gt; | string | Value of the sysfs device attribute, available attributes: class, vendor, device, subsystem_vendor, subsystem_device, sriov_totalvfs, iommu_group/type, iommu/intel-iommu/version | . | storage.block | instance |   |   | Block storage devices present in the system | . |   |   | name | string | Name of the block device | . |   |   | &lt;sysfs-attribute&gt; | string | Sysfs network interface attribute, available attributes: dax, rotational, nr_zones, zoned | . | system.osrelease | attribute |   |   | System identification data from /etc/os-release | . |   |   | &lt;parameter&gt; | string | One parameter from /etc/os-release | . | system.dmiid | attribute |   |   | DMI identification data from /sys/devices/virtual/dmi/id/ | . |   |   | sys_vendor | string | Vendor name from /sys/devices/virtual/dmi/id/sys_vendor | . |   |   | product_name | string | Product name from /sys/devices/virtual/dmi/id/product_name | . | system.name | attribute |   |   | System name information | . |   |   | nodename | string | Name of the kubernetes node object | . | usb.device | instance |   |   | USB devices present in the system | . |   |   | &lt;sysfs-attribute&gt; | string | Value of the sysfs device attribute, available attributes: class, vendor, device, serial | . | rule.matched | attribute |   |   | Previously matched rules | . |   |   | &lt;label-or-var&gt; | string | Label or var from a preceding rule that matched | . Intel RDT flags . | Flag | Description | . | RDTMON | Intel RDT Monitoring Technology | . | RDTCMT | Intel Cache Monitoring (CMT) | . | RDTMBM | Intel Memory Bandwidth Monitoring (MBM) | . | RDTL3CA | Intel L3 Cache Allocation Technology | . | RDTl2CA | Intel L2 Cache Allocation Technology | . | RDTMBA | Intel Memory Bandwidth Allocation (MBA) Technology | . Templating . Rules support template-based creation of labels and vars with the .labelsTemplate and .varsTemplate fields. These makes it possible to dynamically generate labels and vars based on the features that matched. The template must expand into a simple format with &lt;key&gt;=&lt;value&gt; pairs separated by newline. Consider the following example: . labelsTemplate: | {{ range .pci.device }}vendor-{{ .class }}-{{ .device }}.present=true {{ end }} matchFeatures: - feature: pci.device matchExpressions: class: {op: InRegexp, value: [\"^02\"]} vendor: [\"0fff\"] . The rule above will create individual labels feature.node.kubernetes.io/vendor-&lt;class-id&gt;-&lt;device-id&gt;.present=true for each network controller device (device class starting with 02) from vendor 0fff. All the matched features of each feature matcher term under matchFeatures fields are available for the template engine. Matched features can be referenced with {{ .&lt;feature-name&gt; }} in the template, and the available data could be described in yaml as follows: . &lt;key-feature&gt;: - Name: &lt;matched-key&gt; - ... &lt;value-feature&gt;: - Name: &lt;matched-key&gt; Value: &lt;matched-value&gt; - ... &lt;instance-feature&gt;: - &lt;attribute-1-name&gt;: &lt;attribute-1-value&gt; &lt;attribute-2-name&gt;: &lt;attribute-2-value&gt; ... - ... That is, the per-feature data is a list of objects whose data fields depend on the type of the feature: . | for flag features only ‘Name’ is available | for value features ‘Name’ and ‘Value’ are available | for instance features all attributes of the matched instance are available | . A simple example of a template utilizing name and value from an attribute feature: . labelsTemplate: | {{ range .system.osrelease }}system-{{ .Name }}={{ .Value }} {{ end }} matchFeatures: - feature: system.osRelease matchExpressions: ID: {op: Exists} VERSION_ID.major: {op: Exists} . NOTE:If both matchExpressions and matchName for a feature matcher term (see matchFeatures) is specified, the list of matched features (for the template engine) is the union from both of these. NOTE: In case of matchAny is specified, the template is executed separately against each individual matchFeatures field and the final set of labels will be superset of all these separate template expansions. E.g. consider the following: . - name: &lt;name&gt; labelsTemplate: &lt;template&gt; matchFeatures: &lt;matcher#1&gt; matchAny: - matchFeatures: &lt;matcher#2&gt; - matchFeatures: &lt;matcher#3&gt; . In the example above (assuming the overall result is a match) the template would be executed on matcher#1 as well as on matcher#2 and/or matcher#3 (depending on whether both or only one of them match). All the labels from these separate expansions would be created, i.e. the end result would be a union of all the individual expansions. Rule templates use the Golang text/template package along with Sprig functions and all their functionality (e.g. pipelines and functions) can be used. An example template taking use of the built-in len function, advertising the number of PCI network controllers from a specific vendor, and using Sprig’s first, trim and substr to advertise the first one’s class: . labelsTemplate: | num-intel-network-controllers={{ .pci.device | len }} first-intel-network-controllers={{ (.pci.device | first).class | trim | substr 0 63 }} matchFeatures: - feature: pci.device matchExpressions: vendor: {op: In, value: [\"8086\"]} class: {op: In, value: [\"0200\"]} . Imaginative template pipelines are possible, but care must be taken to produce understandable and maintainable rule sets. Backreferences . Rules support referencing the output of preceding rules. This enables sophisticated scenarios where multiple rules are combined together to for more complex heuristics than a single rule can provide. The labels and vars created by the execution of preceding rules are available as a special rule.matched feature. Consider the following configuration: . - name: \"my kernel label rule\" labels: kernel-feature: \"true\" matchFeatures: - feature: kernel.version matchExpressions: major: {op: Gt, value: [\"4\"]} - name: \"my var rule\" vars: nolabel-feature: \"true\" matchFeatures: - feature: cpu.cpuid matchExpressions: AVX512F: {op: Exists} - feature: pci.device matchExpressions: vendor: {op: In, value: [\"0fff\"]} device: {op: In, value: [\"1234\", \"1235\"]} - name: \"my high level feature rule\" labels: high-level-feature: \"true\" matchFeatures: - feature: rule.matched matchExpressions: kernel-feature: {op: IsTrue} nolabel-feature: {op: IsTrue} . The feature.node.kubernetes.io/high-level-feature = true label depends on the two previous rules. Note that when referencing rules across multiple NodeFeatureRule objects attention must be paid to the ordering. NodeFeatureRule objects are processed in alphabetical order (based on their .metadata.name). Examples . Some more configuration examples below. Match certain CPUID features: . - name: \"example cpuid rule\" labels: my-special-cpu-feature: \"true\" matchFeatures: - feature: cpu.cpuid matchExpressions: AESNI: {op: Exists} AVX: {op: Exists} . Require a certain loaded kernel module and OS version: . - name: \"my multi-feature rule\" labels: my-special-multi-feature: \"true\" matchFeatures: - feature: kernel.loadedmodule matchExpressions: e1000: {op: Exists} - feature: system.osrelease matchExpressions: NAME: {op: InRegexp, values: [\"^openSUSE\"]} VERSION_ID.major: {op: Gt, values: [\"14\"]} . Require a loaded kernel module and two specific PCI devices (both of which must be present): . - name: \"my multi-device rule\" labels: my-multi-device-feature: \"true\" matchFeatures: - feature: kernel.loadedmodule matchExpressions: my-driver-module: {op: Exists} - pci.device: vendor: \"0fff\" device: \"1234\" - pci.device: vendor: \"0fff\" device: \"abcd\" . ",
    "url": "/node-feature-discovery/master/usage/customization-guide.html#feature-rule-format",
    
    "relUrl": "/usage/customization-guide.html#feature-rule-format"
  },"17": {
    "doc": "Examples and demos",
    "title": "Examples and demos",
    "content": " ",
    "url": "/node-feature-discovery/master/usage/examples-and-demos.html",
    
    "relUrl": "/usage/examples-and-demos.html"
  },"18": {
    "doc": "Examples and demos",
    "title": "Table of contents",
    "content": ". | Demos . | Usage demo | Demo use case | . | . This page contains usage examples and demos. ",
    "url": "/node-feature-discovery/master/usage/examples-and-demos.html#table-of-contents",
    
    "relUrl": "/usage/examples-and-demos.html#table-of-contents"
  },"19": {
    "doc": "Examples and demos",
    "title": "Demos",
    "content": "Usage demo . Demo use case . A demo on the benefits of using node feature discovery can be found in the source code repository under demo/. ",
    "url": "/node-feature-discovery/master/usage/examples-and-demos.html#demos",
    
    "relUrl": "/usage/examples-and-demos.html#demos"
  },"20": {
    "doc": "Feature Gates",
    "title": "Feature Gates",
    "content": ". Feature gates are a set of key-value pairs that control the behavior of NFD. They are used to enable or disable certain features of NFD. The feature gates are set using the -feature-gates command line flag or featureGates value in the Helm chart. The following feature gates are available: . | Name | Default | Stage | Since | Until | . | NodeFeatureAPI | true | Beta | V0.14 | v0.16 | . | NodeFeatureAPI | true | GA | V0.17 |   | . | DisableAutoPrefix | false | Alpha | V0.16 |   | . | NodeFeatureGroupAPI | false | Alpha | V0.16 |   | . ",
    "url": "/node-feature-discovery/master/reference/feature-gates.html",
    
    "relUrl": "/reference/feature-gates.html"
  },"21": {
    "doc": "Feature Gates",
    "title": "NodeFeatureAPI",
    "content": "The NodeFeatureAPI feature gate enables the Node Feature API. When enabled, NFD will register the Node Feature API with the Kubernetes API server. The Node Feature API is used to expose node-specific hardware and software features to the Kubernetes scheduler. The Node Feature API is a beta feature and is enabled by default. ",
    "url": "/node-feature-discovery/master/reference/feature-gates.html#nodefeatureapi",
    
    "relUrl": "/reference/feature-gates.html#nodefeatureapi"
  },"22": {
    "doc": "Feature Gates",
    "title": "NodeFeatureGroupAPI",
    "content": "The NodeFeatureGroupAPI feature gate enables the Node Feature Group API. When enabled, NFD will register the Node Feature Group API with the Kubernetes API server. The Node Feature Group API is used to create node groups based on hardware and software features. The Node Feature Group API is an alpha feature and is disabled by default. ",
    "url": "/node-feature-discovery/master/reference/feature-gates.html#nodefeaturegroupapi",
    
    "relUrl": "/reference/feature-gates.html#nodefeaturegroupapi"
  },"23": {
    "doc": "Feature Gates",
    "title": "DisableAutoPrefix",
    "content": "The DisableAutoPrefix feature gate controls the automatic prefixing of names. When enabled nfd-master does not automatically add the default feature.node.kubernetes.io/ prefix to unprefixed labels, annotations and extended resources. Automatic prefixing is the default behavior in NFD v0.16 and earlier. Note that enabling the feature gate effectively causes unprefixed names to be filtered out as NFD does not allow unprefixed names of labels, annotations or extended resources. For example, with the DisableAutoPrefix feature gate set to false, a NodeFeatureRule with . labels: foo: bar . will be automatically prefixed, resulting in the node label feature.node.kubernetes.io/foo=bar. However, when DisableAutoPrefix is set to true, no prefix is added, and the label remains as foo=bar. Note that taint keys are not affected by this feature gate. ",
    "url": "/node-feature-discovery/master/reference/feature-gates.html#disableautoprefix",
    
    "relUrl": "/reference/feature-gates.html#disableautoprefix"
  },"24": {
    "doc": "Feature labels",
    "title": "Feature labels",
    "content": " ",
    "url": "/node-feature-discovery/master/usage/features.html",
    
    "relUrl": "/usage/features.html"
  },"25": {
    "doc": "Feature labels",
    "title": "Table of contents",
    "content": ". | Built-in labels . | CPU | Kernel | Memory | Network | PCI | USB | Storage | System | Custom | . | User defined labels | Extended resources | . Features are advertised as labels in the Kubernetes Node object. ",
    "url": "/node-feature-discovery/master/usage/features.html#table-of-contents",
    
    "relUrl": "/usage/features.html#table-of-contents"
  },"26": {
    "doc": "Feature labels",
    "title": "Built-in labels",
    "content": "Label creation in nfd-worker is performed by a set of separate modules called label sources. The core.labelSources configuration option (or -label-sources flag) of nfd-worker controls which sources to enable for label generation. All built-in labels use the feature.node.kubernetes.io label namespace and have the following format. feature.node.kubernetes.io/&lt;feature&gt; = &lt;value&gt; . NOTE: Consecutive runs of nfd-worker will update the labels on a given node. If features are not discovered on a consecutive run, the corresponding label will be removed. This includes any restrictions placed on the consecutive run, such as restricting discovered features with the -label-whitelist flag of nfd-master or core.labelWhiteList option of nfd-worker. CPU . | Feature name | Value | Description | . | cpu-cpuid.&lt;cpuid-flag&gt; | true | CPU capability is supported. NOTE: the capability might be supported but not enabled. | . | cpu-cpuid.&lt;cpuid-attribute&gt; | string | CPU attribute value | . | cpu-hardware_multithreading | true | Hardware multithreading, such as Intel HTT, enabled (number of logical CPUs is greater than physical CPUs) | . | cpu-coprocessor.nx_gzip | true | Nest Accelerator for GZIP is supported(Power). | . | cpu-power.sst_bf.enabled | true | Intel SST-BF (Intel Speed Select Technology - Base frequency) enabled | . | cpu-pstate.status | string | The status of the Intel pstate driver when in use and enabled, either ‘active’ or ‘passive’. | . | cpu-pstate.turbo | bool | Set to ‘true’ if turbo frequencies are enabled in Intel pstate driver, set to ‘false’ if they have been disabled. | . | cpu-pstate.scaling_governor | string | The value of the Intel pstate scaling_governor when in use, either ‘powersave’ or ‘performance’. | . | cpu-cstate.enabled | bool | Set to ‘true’ if cstates are set in the intel_idle driver, otherwise set to ‘false’. Unset if intel_idle cpuidle driver is not active. | . | cpu-security.sgx.enabled | true | Set to ‘true’ if Intel SGX is enabled in BIOS (based on a non-zero sum value of SGX EPC section sizes). | . | cpu-security.se.enabled | true | Set to ‘true’ if IBM Secure Execution for Linux (IBM Z &amp; LinuxONE) is available and enabled (requires /sys/firmware/uv/prot_virt_host facility) | . | cpu-security.tdx.enabled | true | Set to ‘true’ if Intel TDX is available on the host and has been enabled (requires /sys/module/kvm_intel/parameters/tdx). | . | cpu-security.tdx.protected | true | Set to ‘true’ if Intel TDX was used to start the guest node, based on the existence of the “TDX_GUEST” information as part of cpuid features. | . | cpu-security.sev.enabled | true | Set to ‘true’ if ADM SEV is available on the host and has been enabled (requires /sys/module/kvm_amd/parameters/sev). | . | cpu-security.sev.es.enabled | true | Set to ‘true’ if ADM SEV-ES is available on the host and has been enabled (requires /sys/module/kvm_amd/parameters/sev_es). | . | cpu-security.sev.snp.enabled | true | Set to ‘true’ if ADM SEV-SNP is available on the host and has been enabled (requires /sys/module/kvm_amd/parameters/sev_snp). | . | cpu-model.vendor_id | string | Comparable CPU vendor ID. | . | cpu-model.family | int | CPU family. | . | cpu-model.id | int | CPU model number. | . The CPU label source is configurable, see worker configuration and sources.cpu configuration options for details. X86 CPUID flags (partial list) . | Flag | Description | . | ADX | Multi-Precision Add-Carry Instruction Extensions (ADX) | . | AESNI | Advanced Encryption Standard (AES) New Instructions (AES-NI) | . | APX_F | Intel Advanced Performance Extensions (APX) | . | AVX10 | Intel Advanced Vector Extensions 10 (AVX10) | . | AVX10_256, AVX10_512 | Intel AVX10 256-bit and 512-bit vector support | . | AVX | Advanced Vector Extensions (AVX) | . | AVX2 | Advanced Vector Extensions 2 (AVX2) | . | AVXIFMA | AVX-IFMA instructions | . | AVXVNNI | AVX (VEX encoded) VNNI neural network instructions | . | AMXBF16 | Advanced Matrix Extension, tile multiplication operations on BFLOAT16 numbers | . | AMXCOMPLEX | Advanced Matrix Extension, tile computational operations on complex numbers | . | AMXINT8 | Advanced Matrix Extension, tile multiplication operations on 8-bit integers | . | AMXFP16 | Advanced Matrix Extension, tile multiplication operations on FP16 numbers | . | AMXFP8 | Advanced Matrix Extension, tile multiplication operations on FP8 numbers | . | AMXTILE | Advanced Matrix Extension, base tile architecture support | . | AMXTF32 | Advanced Matrix Extension, matrix multiplication of TF32 tiles into packed single precision tile | . | AVX512BF16 | AVX-512 BFLOAT16 instructions | . | AVX512BITALG | AVX-512 bit Algorithms | . | AVX512BW | AVX-512 byte and word Instructions | . | AVX512CD | AVX-512 conflict detection instructions | . | AVX512DQ | AVX-512 doubleword and quadword instructions | . | AVX512ER | AVX-512 exponential and reciprocal instructions | . | AVX512F | AVX-512 foundation | . | AVX512FP16 | AVX-512 FP16 instructions | . | AVX512IFMA | AVX-512 integer fused multiply-add instructions | . | AVX512PF | AVX-512 prefetch instructions | . | AVX512VBMI | AVX-512 vector bit manipulation instructions | . | AVX512VBMI2 | AVX-512 vector bit manipulation instructions, version 2 | . | AVX512VL | AVX-512 vector length extensions | . | AVX512VNNI | AVX-512 vector neural network instructions | . | AVX512VP2INTERSECT | AVX-512 intersect for D/Q | . | AVX512VPOPCNTDQ | AVX-512 vector population count doubleword and quadword | . | AVXNECONVERT | AVX-NE-CONVERT instructions | . | AVXVNNIINT8 | AVX-VNNI-INT8 instructions | . | AVXVNNIINT16 | AVX-VNNI-INT16 instructions | . | CMPCCXADD | CMPCCXADD instructions | . | ENQCMD | Enqueue Command | . | GFNI | Galois Field New Instructions | . | HYPERVISOR | Running under hypervisor | . | MSRLIST | Read/Write List of Model Specific Registers | . | PREFETCHI | PREFETCHIT0/1 instructions | . | VAES | AVX-512 vector AES instructions | . | VPCLMULQDQ | Carry-less multiplication quadword | . | WRMSRNS | Non-Serializing Write to Model Specific Register | . By default, the following CPUID flags have been blacklisted: AVX10 (use AVX10_VERSION instead), BMI1, BMI2, CLMUL, CMOV, CX16, ERMS, F16C, HTT, LZCNT, MMX, MMXEXT, NX, POPCNT, RDRAND, RDSEED, RDTSCP, SGX, SSE, SSE2, SSE3, SSE4, SSE42, SSSE3 and TDX_GUEST. See sources.cpu configuration options to change the behavior. See the full list in github.com/klauspost/cpuid. X86 CPUID attributes . | Attribute | Description | . | AVX10_VERSION | AVX10 vector ISA version (if supported) | . Arm CPUID flags (partial list) . | Flag | Description | . | IDIVA | Integer divide instructions available in ARM mode | . | IDIVT | Integer divide instructions available in Thumb mode | . | THUMB | Thumb instructions | . | FASTMUL | Fast multiplication | . | VFP | Vector floating point instruction extension (VFP) | . | VFPv3 | Vector floating point extension v3 | . | VFPv4 | Vector floating point extension v4 | . | VFPD32 | VFP with 32 D-registers | . | HALF | Half-word loads and stores | . | EDSP | DSP extensions | . | NEON | NEON SIMD instructions | . | LPAE | Large Physical Address Extensions | . Arm64 CPUID flags (partial list) . | Flag | Description | . | AES | Announcing the Advanced Encryption Standard | . | EVSTRM | Event Stream Frequency Features | . | FPHP | Half Precision(16bit) Floating Point Data Processing Instructions | . | ASIMDHP | Half Precision(16bit) Asimd Data Processing Instructions | . | ATOMICS | Atomic Instructions to the A64 | . | ASIMRDM | Support for Rounding Double Multiply Add/Subtract | . | PMULL | Optional Cryptographic and CRC32 Instructions | . | JSCVT | Perform Conversion to Match Javascript | . | DCPOP | Persistent Memory Support | . Kernel . | Feature | Value | Description | . | kernel-config.&lt;option&gt; | true | Kernel config option is enabled (set ‘y’ or ‘m’). Default options are NO_HZ, NO_HZ_IDLE, NO_HZ_FULL and PREEMPT | . | kernel-selinux.enabled | true | Selinux is enabled on the node | . | kernel-version.full | string | Full kernel version as reported by /proc/sys/kernel/osrelease (e.g. ‘4.5.6-7-g123abcde’) | . | kernel-version.major | string | First component of the kernel version (e.g. ‘4’) | . | kernel-version.minor | string | Second component of the kernel version (e.g. ‘5’) | . | kernel-version.revision | string | Third component of the kernel version (e.g. ‘6’) | . The kernel label source is configurable, see worker configuration and sources.kernel configuration options for details. Memory . | Feature | Value | Description | . | memory-numa | true | Multiple memory nodes i.e. NUMA architecture detected | . | memory-nv.present | true | NVDIMM device(s) are present | . | memory-nv.dax | true | NVDIMM region(s) configured in DAX mode are present | . | memory-swap.enabled | true | Swap is enabled on the node | . Network . | Feature | Value | Description | . | network-sriov.capable | true | Single Root Input/Output Virtualization (SR-IOV) enabled Network Interface Card(s) present | . | network-sriov.configured | true | SR-IOV virtual functions have been configured | . PCI . | Feature | Value | Description | . | pci-&lt;device label&gt;.present | true | PCI device is detected | . | pci-&lt;device label&gt;.sriov.capable | true | Single Root Input/Output Virtualization (SR-IOV) enabled PCI device present | . |   |   |   | . &lt;device label&gt; is format is configurable and set to &lt;class&gt;_&lt;vendor&gt; by default. For more more details about configuration of the pci labels, see sources.pci options and worker configuration instructions. USB . | Feature | Value | Description | . | usb-&lt;device label&gt;.present | true | USB device is detected | . &lt;device label&gt; is format is configurable and set to &lt;class&gt;_&lt;vendor&gt;_&lt;device&gt; by default. For more more details about configuration of the usb labels, see sources.usb options and worker configuration instructions. Storage . | Feature | Value | Description | . | storage-nonrotationaldisk | true | Non-rotational disk, like SSD, is present in the node | . System . | Feature | Value | Description | . | system-os_release.ID | string | Operating system identifier | . | system-os_release.VERSION_ID | string | Operating system version identifier (e.g. ‘6.7’) | . | system-os_release.VERSION_ID.major | string | First component of the OS version id (e.g. ‘6’) | . | system-os_release.VERSION_ID.minor | string | Second component of the OS version id (e.g. ‘7’) | . Custom . The custom label source is designed for creating user defined labels. However, it has a few statically defined built-in labels: . | Feature | Value | Description | . | custom-rdma.capable | true | The node has an RDMA capable Network adapter | . | custom-rdma.enabled | true | The node has the needed RDMA modules loaded to run RDMA traffic | . |   |   |   | . ",
    "url": "/node-feature-discovery/master/usage/features.html#built-in-labels",
    
    "relUrl": "/usage/features.html#built-in-labels"
  },"27": {
    "doc": "Feature labels",
    "title": "User defined labels",
    "content": "NFD has many extension points for creating vendor and application specific labels. See the customization guide for detailed documentation. ",
    "url": "/node-feature-discovery/master/usage/features.html#user-defined-labels",
    
    "relUrl": "/usage/features.html#user-defined-labels"
  },"28": {
    "doc": "Feature labels",
    "title": "Extended resources",
    "content": "NFD is able to create extended resources, see the NodeFeatureRule CRD and its extendedResources field for more details. Note that NFD is not a replacement for the usage of device plugins. An example use-case for extended resources could be based on custom feature (created e.g. with feature files that exposes the node SGX EPC memory section size. This value will then be turned into an extended resource of the node, allowing PODs to request that resource and the Kubernetes scheduler to schedule such PODs to only those nodes which have a sufficient capacity of said resource left. ",
    "url": "/node-feature-discovery/master/usage/features.html#extended-resources",
    
    "relUrl": "/usage/features.html#extended-resources"
  },"29": {
    "doc": "Garbage Collector Cmdline Reference",
    "title": "NFD-GC Commandline Flags",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/gc-commandline-reference.html#nfd-gc-commandline-flags",
    
    "relUrl": "/reference/gc-commandline-reference.html#nfd-gc-commandline-flags"
  },"30": {
    "doc": "Garbage Collector Cmdline Reference",
    "title": "Table of Contents",
    "content": ". | -h, -help | -version | -gc-interval | . To quickly view available command line flags execute nfd-gc -help. In a docker container: . docker run gcr.io/k8s-staging-nfd/node-feature-discovery:master \\ nfd-gc -help . -h, -help . Print usage and exit. -version . Print version and exit. -gc-interval . The -gc-interval specifies the interval between periodic garbage collector runs. Default: 1h . Example: . nfd-gc -gc-interval=1h . ",
    "url": "/node-feature-discovery/master/reference/gc-commandline-reference.html#table-of-contents",
    
    "relUrl": "/reference/gc-commandline-reference.html#table-of-contents"
  },"31": {
    "doc": "Garbage Collector Cmdline Reference",
    "title": "Garbage Collector Cmdline Reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/gc-commandline-reference.html",
    
    "relUrl": "/reference/gc-commandline-reference.html"
  },"32": {
    "doc": "Helm",
    "title": "Deployment with Helm",
    "content": " ",
    "url": "/node-feature-discovery/master/deployment/helm.html#deployment-with-helm",
    
    "relUrl": "/deployment/helm.html#deployment-with-helm"
  },"33": {
    "doc": "Helm",
    "title": "Table of contents",
    "content": ". | Deployment with Helm . | Prerequisites | Deployment | Configuration | Upgrading the chart . | From v0.7 and older | From v0.8 - v0.11 | From v0.12 - v0.13 | From v0.14+ | . | Uninstalling the chart | Chart parameters . | General parameters | Master pod parameters | Worker pod parameters | Topology updater parameters | Garbage collector parameters | . | . | . Node Feature Discovery provides a Helm chart to manage its deployment. NOTE: NFD is not ideal for other Helm charts to depend on as that may result in multiple parallel NFD deployments in the same cluster which is not fully supported by the NFD Helm chart. ",
    "url": "/node-feature-discovery/master/deployment/helm.html#table-of-contents",
    
    "relUrl": "/deployment/helm.html#table-of-contents"
  },"34": {
    "doc": "Helm",
    "title": "Prerequisites",
    "content": "Helm package manager should be installed. ",
    "url": "/node-feature-discovery/master/deployment/helm.html#prerequisites",
    
    "relUrl": "/deployment/helm.html#prerequisites"
  },"35": {
    "doc": "Helm",
    "title": "Deployment",
    "content": "To install the latest stable version: . export NFD_NS=node-feature-discovery helm repo add nfd https://kubernetes-sigs.github.io/node-feature-discovery/charts helm repo update helm install nfd/node-feature-discovery --namespace $NFD_NS --create-namespace --generate-name . To install the latest development version you need to clone the NFD Git repository and install from there. git clone https://github.com/kubernetes-sigs/node-feature-discovery/ cd node-feature-discovery/deployment/helm export NFD_NS=node-feature-discovery helm install node-feature-discovery ./node-feature-discovery/ --namespace $NFD_NS --create-namespace . See the configuration section below for instructions how to alter the deployment parameters. ",
    "url": "/node-feature-discovery/master/deployment/helm.html#deployment",
    
    "relUrl": "/deployment/helm.html#deployment"
  },"36": {
    "doc": "Helm",
    "title": "Configuration",
    "content": "You can override values from values.yaml and provide a file with custom values: . export NFD_NS=node-feature-discovery helm install nfd/node-feature-discovery -f &lt;path/to/custom/values.yaml&gt; --namespace $NFD_NS --create-namespace . To specify each parameter separately you can provide them to helm install command: . export NFD_NS=node-feature-discovery helm install nfd/node-feature-discovery --set nameOverride=NFDinstance --set master.replicaCount=2 --namespace $NFD_NS --create-namespace . ",
    "url": "/node-feature-discovery/master/deployment/helm.html#configuration",
    
    "relUrl": "/deployment/helm.html#configuration"
  },"37": {
    "doc": "Helm",
    "title": "Upgrading the chart",
    "content": "To upgrade the node-feature-discovery deployment to master via Helm. From v0.7 and older . Please see the uninstallation guide. And then follow the standard deployment instructions. From v0.8 - v0.11 . Helm deployment of NFD was introduced in v0.8.0. export NFD_NS=node-feature-discovery # Uninstall the old NFD deployment helm uninstall node-feature-discovery --namespace $NFD_NS # Update Helm repository helm repo update # Install the new NFD deployment helm upgrade --install node-feature-discovery nfd/node-feature-discovery --namespace $NFD_NS --set master.enable=false # Wait for NFD Worker to be ready kubectl wait --timeout=-1s --for=condition=ready pod -l app.kubernetes.io/name=node-feature-discovery --namespace $NFD_NS # Enable the NFD Master helm upgrade --install node-feature-discovery nfd/node-feature-discovery --namespace $NFD_NS --set master.enable=true . From v0.12 - v0.13 . In v0.12 the NodeFeature CRD was introduced as experimental. The API was not enabled by default. export NFD_NS=node-feature-discovery # Update Helm repository helm repo update # Install and upgrade CRD's kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/node-feature-discovery/master/deployment/base/nfd-crds/nfd-api-crds.yaml # Install the new NFD deployment helm upgrade node-feature-discovery nfd/node-feature-discovery --namespace $NFD_NS --set master.enable=false # Wait for NFD Worker to be ready kubectl wait --timeout=-1s --for=condition=ready pod -l app.kubernetes.io/name=node-feature-discovery --namespace $NFD_NS # Enable the NFD Master helm upgrade node-feature-discovery nfd/node-feature-discovery --namespace $NFD_NS --set master.enable=true . From v0.14+ . As of version v0.14 the Helm chart is the primary deployment method for NFD, and the CRD NodeFeature is enabled by default. export NFD_NS=node-feature-discovery # Update Helm repository helm repo update # Install and upgrade CRD's kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/node-feature-discovery/master/deployment/base/nfd-crds/nfd-api-crds.yaml # Install the new NFD deployment helm upgrade node-feature-discovery nfd/node-feature-discovery --namespace $NFD_NS . ",
    "url": "/node-feature-discovery/master/deployment/helm.html#upgrading-the-chart",
    
    "relUrl": "/deployment/helm.html#upgrading-the-chart"
  },"38": {
    "doc": "Helm",
    "title": "Uninstalling the chart",
    "content": "To uninstall the node-feature-discovery deployment: . export NFD_NS=node-feature-discovery helm uninstall node-feature-discovery --namespace $NFD_NS . The command removes all the Kubernetes components associated with the chart and deletes the release. It also runs a post-delete hook that cleans up the nodes of all labels, annotations, taints and extended resources that were created by NFD. ",
    "url": "/node-feature-discovery/master/deployment/helm.html#uninstalling-the-chart",
    
    "relUrl": "/deployment/helm.html#uninstalling-the-chart"
  },"39": {
    "doc": "Helm",
    "title": "Chart parameters",
    "content": "To tailor the deployment of the Node Feature Discovery to your needs following Chart parameters are available. General parameters . | Name | Type | Default | Description | . | image.repository | string | gcr.io/k8s-staging-nfd/node-feature-discovery | NFD image repository | . | image.tag | string | master | NFD image tag | . | image.pullPolicy | string | Always | Image pull policy | . | imagePullSecrets | array | [] | ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec. More info. | . | nameOverride | string |   | Override the name of the chart | . | fullnameOverride | string |   | Override a default fully qualified app name | . | featureGates.NodeFeatureGroupAPI | bool | false | Enable the NodeFeatureGroup CRD API. | . | featureGates.DisableAutoPrefix | bool | false | Enable DisableAutoPrefix feature gate. Disables automatic prefixing of unprefixed labels, annotations and extended resources. | . | prometheus.enable | bool | false | Specifies whether to expose metrics using prometheus operator | . | prometheus.labels | dict | {} | Specifies labels for use with the prometheus operator to control how it is selected | . | prometheus.scrapeInterval | string | 10s | Specifies the interval by which metrics are scraped | . | priorityClassName | string |   | The name of the PriorityClass to be used for the NFD pods. | . | postDeleteCleanup | bool | true | Controls behavior of post-delete hook. | . Metrics are configured to be exposed using prometheus operator API’s by default. If you want to expose metrics using the prometheus operator API’s you need to install the prometheus operator in your cluster. Master pod parameters . | Name | Type | Default | Description | . | master.* | dict |   | NFD master deployment configuration | . | master.enable | bool | true | Specifies whether nfd-master should be deployed | . | master.hostNetwork | bool | false | Specifies whether to enable or disable running the container in the host’s network namespace | . | master.port | integer | 8080 | Port on which to serve http for metrics and healthz endpoints. | . | master.instance | string |   | Instance name. Used to separate annotation namespaces for multiple parallel deployments | . | master.resyncPeriod | string |   | NFD API controller resync period. | . | master.extraLabelNs | array | [] | List of allowed extra label namespaces | . | master.enableTaints | bool | false | Specifies whether to enable or disable node tainting | . | master.replicaCount | integer | 1 | Number of desired pods. This is a pointer to distinguish between explicit zero and not specified | . | master.podSecurityContext | dict | {} | PodSecurityContext holds pod-level security attributes and common container settings | . | master.securityContext | dict | {} | Container security settings | . | master.serviceAccount.create | bool | true | Specifies whether a service account should be created | . | master.serviceAccount.annotations | dict | {} | Annotations to add to the service account | . | master.serviceAccount.name | string |   | The name of the service account to use. If not set and create is true, a name is generated using the fullname template | . | master.rbac.create | bool | true | Specifies whether to create RBAC configuration for nfd-master | . | master.resources.limits | dict | {memory: 4Gi} | NFD master pod resources limits | . | master.resources.requests | dict | {cpu: 100m, memory: 128Mi} | NFD master pod resources requests. See [0] for more info | . | master.podDisruptionBudget.enable | bool | false | Specifies whether to create a podDisruptionBudget configuration for nfd-master | . | master.podDisruptionBudget.minAvailable | integer | 1 | Specifies minAvailable for the podDisruptionBudget configuration for nfd-master | . | master.podDisruptionBudget.maxUnavailable | integer | NULL | Specifies maxUnavailable for the podDisruptionBudget configuration for nfd-master | . | master.podDisruptionBudget.unhealthyPodEvictionPolicy | string | AlwaysAllow | Specifies unhealthyPodEvictionPolicy for the podDisruptionBudget configuration for nfd-master | . | master.tolerations | dict | Schedule to control-plane node | NFD master pod tolerations | . | master.annotations | dict | {} | NFD master pod annotations | . | master.affinity | dict |   | NFD master pod required node affinity | . | master.deploymentAnnotations | dict | {} | NFD master deployment annotations | . | master.nfdApiParallelism | integer | 10 | Specifies the maximum number of concurrent node updates. | . | master.config | dict |   | NFD master configuration | . | master.extraArgs | array | [] | Additional command line arguments to pass to nfd-master | . | master.extraEnvs | array | [] | Additional environment variables to pass to nfd-master | . | master.revisionHistoryLimit | integer |   | Specify how many old ReplicaSets for this Deployment you want to retain. revisionHistoryLimit | . | master.startupProbe.initialDelaySecond s | integer | 0 (by Kubernetes) | Specifies the number of seconds after the container has started before startup probes are initiated. | . | master.startupProbe.failureThreshold | integer | 30 | Specifies the number of consecutive failures of startup probes before considering the pod as not ready. | . | master.startupProbe.periodSeconds | integer | 10 (by Kubernetes) | Specifies how often (in seconds) to perform the startup probe. | . | master.startupProbe.timeoutSeconds | integer | 1 (by Kubernetes) | Specifies the number of seconds after which the probe times out. | . | master.livenessProbe.initialDelaySeconds | integer | 0 (by Kubernetes) | Specifies the number of seconds after the container has started before liveness probes are initiated. | . | master.livenessProbe.failureThreshold | integer | 3 (by Kubernetes) | Specifies the number of consecutive failures of liveness probes before considering the pod as not ready. | . | master.livenessProbe.periodSeconds | integer | 10 (by Kubernetes) | Specifies how often (in seconds) to perform the liveness probe. | . | master.livenessProbe.timeoutSeconds | integer | 1 (by Kubernetes) | Specifies the number of seconds after which the probe times out. | . | master.readinessProbe.initialDelaySeconds | integer | 0 (by Kubernetes) | Specifies the number of seconds after the container has started before readiness probes are initiated. | . | master.readinessProbe.failureThreshold | integer | 10 | Specifies the number of consecutive failures of readiness probes before considering the pod as not ready. | . | master.readinessProbe.periodSeconds | integer | 10 (by Kubernetes) | Specifies how often (in seconds) to perform the readiness probe. | . | master.readinessProbe.timeoutSeconds | integer | 1 (by Kubernetes) | Specifies the number of seconds after which the probe times out. | . | master.readinessProbe.successThreshold | integer | 1 (by Kubernetes) | Specifies the number of consecutive successes of readiness probes before considering the pod as ready. | . | master.dnsPolicy | array | ClusterFirstWithHostNet | NFD master pod dnsPolicy | . [0] Additional info for master.resources.requests: You may want to use the same value for requests.memory and limits.memory. The “requests” value affects scheduling to accommodate pods on nodes. If there is a large difference between “requests” and “limits” and nodes experience memory pressure, the kernel may invoke the OOM Killer, even if the memory does not exceed the “limits” threshold. This can cause unexpected pod evictions. Memory cannot be compressed and once allocated to a pod, it can only be reclaimed by killing the pod. Natan Yellin 22/09/2022 that discusses this issue. Worker pod parameters . | Name | Type | Default | Description | . | worker.* | dict |   | NFD worker daemonset configuration | . | worker.enable | bool | true | Specifies whether nfd-worker should be deployed | . | worker.hostNetwork | bool | false | Specifies whether to enable or disable running the container in the host’s network namespace | . | worker.port | int | 8080 | Port on which to serve http for metrics and healthz endpoints. | . | worker.config | dict |   | NFD worker configuration | . | worker.podSecurityContext | dict | {} | PodSecurityContext holds pod-level security attributes and common container settins | . | worker.securityContext | dict | {} | Container security settings | . | worker.serviceAccount.create | bool | true | Specifies whether a service account for nfd-worker should be created | . | worker.serviceAccount.annotations | dict | {} | Annotations to add to the service account for nfd-worker | . | worker.serviceAccount.name | string |   | The name of the service account to use for nfd-worker. If not set and create is true, a name is generated using the fullname template (suffixed with -worker) | . | worker.rbac.create | bool | true | Specifies whether to create RBAC configuration for nfd-worker | . | worker.mountUsrSrc | bool | false | Specifies whether to allow users to mount the hostpath /user/src. Does not work on systems without /usr/src AND a read-only /usr | . | worker.resources.limits | dict | {memory: 512Mi} | NFD worker pod resources limits | . | worker.resources.requests | dict | {cpu: 5m, memory: 64Mi} | NFD worker pod resources requests | . | worker.nodeSelector | dict | {} | NFD worker pod node selector | . | worker.tolerations | dict | {} | NFD worker pod node tolerations | . | worker.priorityClassName | string |   | NFD worker pod priority class | . | worker.annotations | dict | {} | NFD worker pod annotations | . | worker.daemonsetAnnotations | dict | {} | NFD worker daemonset annotations | . | worker.extraArgs | array | [] | Additional command line arguments to pass to nfd-worker | . | worker.extraEnvs | array | [] | Additional environment variables to pass to nfd-worker | . | worker.revisionHistoryLimit | integer |   | Specify how many old ControllerRevisions for this DaemonSet you want to retain. revisionHistoryLimit | . | worker.livenessProbe.initialDelaySeconds | integer | 10 | Specifies the number of seconds after the container has started before liveness probes are initiated. | . | worker.livenessProbe.failureThreshold | integer | 3 (by Kubernetes) | Specifies the number of consecutive failures of liveness probes before considering the pod as not ready. | . | worker.livenessProbe.periodSeconds | integer | 10 (by Kubernetes) | Specifies how often (in seconds) to perform the liveness probe. | . | worker.livenessProbe.timeoutSeconds | integer | 1 (by Kubernetes) | Specifies the number of seconds after which the probe times out. | . | worker.readinessProbe.initialDelaySeconds | integer | 5 | Specifies the number of seconds after the container has started before readiness probes are initiated. | . | worker.readinessProbe.failureThreshold | integer | 10 | Specifies the number of consecutive failures of readiness probes before considering the pod as not ready. | . | worker.readinessProbe.periodSeconds | integer | 10 (by Kubernetes) | Specifies how often (in seconds) to perform the readiness probe. | . | worker.readinessProbe.timeoutSeconds | integer | 1 (by Kubernetes) | Specifies the number of seconds after which the probe times out. | . | worker.readinessProbe.successThreshold | integer | 1 (by Kubernetes) | Specifies the number of consecutive successes of readiness probes before considering the pod as ready. | . | worker.dnsPolicy | array | ClusterFirstWithHostNet | NFD worker pod dnsPolicy | . | worker.updateStrategy | dict | {} | Daemonset updateStrategy | . Topology updater parameters . | Name | Type | Default | Description | . | topologyUpdater.* | dict |   | NFD Topology Updater configuration | . | topologyUpdater.enable | bool | false | Specifies whether the NFD Topology Updater should be created | . | topologyUpdater.hostNetwork | bool | false | Specifies whether to enable or disable running the container in the host’s network namespace | . | topologyUpdater.createCRDs | bool | false | Specifies whether the NFD Topology Updater CRDs should be created | . | topologyUpdater.serviceAccount.create | bool | true | Specifies whether the service account for topology updater should be created | . | topologyUpdater.serviceAccount.annotations | dict | {} | Annotations to add to the service account for topology updater | . | topologyUpdater.serviceAccount.name | string |   | The name of the service account for topology updater to use. If not set and create is true, a name is generated using the fullname template and -topology-updater suffix | . | topologyUpdater.rbac.create | bool | true | Specifies whether to create RBAC configuration for topology updater | . | topologyUpdater.port | integer | 8080 | Port on which to serve http for metrics and healthz endpoints. | . | topologyUpdater.kubeletConfigPath | string | ”” | Specifies the kubelet config host path | . | topologyUpdater.kubeletPodResourcesSockPath | string | ”” | Specifies the kubelet sock path to read pod resources | . | topologyUpdater.updateInterval | string | 60s | Time to sleep between CR updates. Non-positive value implies no CR update. | . | topologyUpdater.watchNamespace | string | * | Namespace to watch pods, * for all namespaces | . | topologyUpdater.podSecurityContext | dict | {} | PodSecurityContext holds pod-level security attributes and common container sett | . | topologyUpdater.securityContext | dict | {} | Container security settings | . | topologyUpdater.resources.limits | dict | {memory: 60Mi} | NFD Topology Updater pod resources limits | . | topologyUpdater.resources.requests | dict | {cpu: 50m, memory: 40Mi} | NFD Topology Updater pod resources requests | . | topologyUpdater.nodeSelector | dict | {} | Topology updater pod node selector | . | topologyUpdater.tolerations | dict | {} | Topology updater pod node tolerations | . | topologyUpdater.annotations | dict | {} | Topology updater pod annotations | . | topologyUpdater.daemonsetAnnotations | dict | {} | Topology updater daemonset annotations | . | topologyUpdater.affinity | dict | {} | Topology updater pod affinity | . | topologyUpdater.config | dict |   | configuration | . | topologyUpdater.podSetFingerprint | bool | true | Enables compute and report of pod fingerprint in NRT objects. | . | topologyUpdater.kubeletStateDir | string | /var/lib/kubelet | Specifies kubelet state directory path for watching state and checkpoint files. Empty value disables kubelet state tracking. | . | topologyUpdater.extraArgs | array | [] | Additional command line arguments to pass to nfd-topology-updater | . | topologyUpdater.extraEnvs | array | [] | Additional environment variables to pass to nfd-topology-updater | . | topologyUpdater.revisionHistoryLimit | integer |   | Specify how many old ControllerRevisions for this DaemonSet you want to retain. revisionHistoryLimit | . | topologyUpdater.livenessProbe.initialDelaySeconds | integer | 10 | Specifies the number of seconds after the container has started before liveness probes are initiated. | . | topologyUpdater.livenessProbe.failureThreshold | integer | 3 (by Kubernetes) | Specifies the number of consecutive failures of liveness probes before considering the pod as not ready. | . | topologyUpdater.livenessProbe.periodSeconds | integer | 10 (by Kubernetes) | Specifies how often (in seconds) to perform the liveness probe. | . | topologyUpdater.livenessProbe.timeoutSeconds | integer | 1 (by Kubernetes) | Specifies the number of seconds after which the probe times out. | . | topologyUpdater.readinessProbe.initialDelaySeconds | integer | 5 | Specifies the number of seconds after the container has started before readiness probes are initiated. | . | topologyUpdater.readinessProbe.failureThreshold | integer | 10 | Specifies the number of consecutive failures of readiness probes before considering the pod as not ready. | . | topologyUpdater.readinessProbe.periodSeconds | integer | 10 (by Kubernetes) | Specifies how often (in seconds) to perform the readiness probe. | . | topologyUpdater.readinessProbe.timeoutSeconds | integer | 1 (by Kubernetes) | Specifies the number of seconds after which the probe times out. | . | topologyUpdater.readinessProbe.successThreshold | integer | 1 (by Kubernetes) | Specifies the number of consecutive successes of readiness probes before considering the pod as ready. | . | topologyUpdater.dnsPolicy | array | ClusterFirstWithHostNet | Topology updater pod dnsPolicy | . Garbage collector parameters . | Name | Type | Default | Description | . | gc.* | dict |   | NFD Garbage Collector configuration | . | gc.enable | bool | true | Specifies whether the NFD Garbage Collector should be created | . | gc.hostNetwork | bool | false | Specifies whether to enable or disable running the container in the host’s network namespace | . | gc.serviceAccount.create | bool | true | Specifies whether the service account for garbage collector should be created | . | gc.serviceAccount.annotations | dict | {} | Annotations to add to the service account for garbage collector | . | gc.serviceAccount.name | string |   | The name of the service account for garbage collector to use. If not set and create is true, a name is generated using the fullname template and -gc suffix | . | gc.rbac.create | bool | true | Specifies whether to create RBAC configuration for garbage collector | . | gc.interval | string | 1h | Time between periodic garbage collector runs | . | gc.podSecurityContext | dict | {} | PodSecurityContext holds pod-level security attributes and common container settings | . | gc.resources.limits | dict | {memory: 1Gi} | NFD Garbage Collector pod resources limits | . | gc.resources.requests | dict | {cpu: 10m, memory: 128Mi} | NFD Garbage Collector pod resources requests | . | gc.podDisruptionBudget.enable | bool | false | Specifies whether to create a podDisruptionBudget configuration for nfd-gc | . | gc.podDisruptionBudget.minAvailable | integer | 1 | Specifies minAvailable for the podDisruptionBudget configuration for nfd-gc | . | gc.podDisruptionBudget.maxUnavailable | integer | NULL | Specifies maxUnavailable for the podDisruptionBudget configuration for nfd-gc | . | gc.podDisruptionBudget.unhealthyPodEvictionPolicy | string | AlwaysAllow | Specifies unhealthyPodEvictionPolicy for the podDisruptionBudget configuration for nfd-gc | . | gc.metricsPort | integer | 8081 | Port on which to serve Prometheus metrics. DEPRECATED: will be replaced by gc.port in NFD v0.18. | . | gc.nodeSelector | dict | {} | Garbage collector pod node selector | . | gc.tolerations | dict | {} | Garbage collector pod node tolerations | . | gc.annotations | dict | {} | Garbage collector pod annotations | . | gc.deploymentAnnotations | dict | {} | Garbage collector deployment annotations | . | gc.affinity | dict | {} | Garbage collector pod affinity | . | gc.extraArgs | array | [] | Additional command line arguments to pass to nfd-gc | . | gc.extraEnvs | array | [] | Additional environment variables to pass to nfd-gc | . | gc.revisionHistoryLimit | integer |   | Specify how many old ReplicaSets for this Deployment you want to retain. revisionHistoryLimit | . | gc.dnsPolicy | array | ClusterFirstWithHostNet | Garbage collector pod dnsPolicy | . ",
    "url": "/node-feature-discovery/master/deployment/helm.html#chart-parameters",
    
    "relUrl": "/deployment/helm.html#chart-parameters"
  },"40": {
    "doc": "Helm",
    "title": "Helm",
    "content": " ",
    "url": "/node-feature-discovery/master/deployment/helm.html",
    
    "relUrl": "/deployment/helm.html"
  },"41": {
    "doc": "Image Compatibility Artifact",
    "title": "Image Compatibility Artifact",
    "content": " ",
    "url": "/node-feature-discovery/master/usage/image-compatibility.html",
    
    "relUrl": "/usage/image-compatibility.html"
  },"42": {
    "doc": "Image Compatibility Artifact",
    "title": "Table of contents",
    "content": ". | Image Compatibility . | Compatibility Specification | Schema | OCI Artifact | Validate the host against the image compatibility specification | Validate the k8s cluster node with the validate-image Job | . | . ",
    "url": "/node-feature-discovery/master/usage/image-compatibility.html#table-of-contents",
    
    "relUrl": "/usage/image-compatibility.html#table-of-contents"
  },"43": {
    "doc": "Image Compatibility Artifact",
    "title": "Image Compatibility",
    "content": "Image Compatibility is in the experimental v1alpha1 version. Image compatibility metadata enables container image authors to define their image requirements using a spec similar to Node Feature Groups. This complementary solution allows features discovered on nodes to be matched directly from images. As a result, container requirements become discoverable and programmable, supporting various consumers and use cases where applications need a specific compatible environment. Compatibility Specification . The compatibility specification is a list of compatibility objects that contain rules similar to Node Feature Groups, along with additional fields to control the execution of validation between the image and the host. Schema . | version - string This REQUIRED property specifies the version of the API in use. | compatibilities - array of object This REQUIRED property is a list of compatibility sets. | rules - object This REQUIRED property is a reference to the spec of the NodeFeatureGroup API. The spec allows image requirements to be described using the features discovered from NFD sources. For more details, please refer to the documentation. | weight - int This OPTIONAL property specifies the node affinity weight. | tag - string This OPTIONAL property allows for the grouping or separation of compatibility sets. | description - string This OPTIONAL property provides a brief description of a compatibility set. | . | . Example . version: v1alpha1 compatibilities: - description: \"My image requirements\" rules: - name: \"kernel and cpu\" matchFeatures: - feature: kernel.loadedmodule matchExpressions: vfio-pci: {op: Exists} - feature: cpu.model matchExpressions: vendor_id: {op: In, value: [\"Intel\", \"AMD\"]} - name: \"one of available nics\" matchAny: - matchFeatures: - feature: pci.device matchExpressions: vendor: {op: In, value: [\"0eee\"]} class: {op: In, value: [\"0200\"]} - matchFeatures: - feature: pci.device matchExpressions: vendor: {op: In, value: [\"0fff\"]} class: {op: In, value: [\"0200\"]} . OCI Artifact . An OCI artifact is used to store image compatibility metadata. The artifact can be associated with a specific image through the subject field and pushed to the registry along with the image. Example manifest: . { \"schemaVersion\": 2, \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\", \"artifactType\": \"application/vnd.nfd.image-compatibility.v1alpha1\", \"config\": { \"mediaType\": \"application/vnd.oci.empty.v1+json\", \"digest\": \"sha256:44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\", \"size\": 2 }, \"layers\": [ { \"mediaType\": \"application/vnd.nfd.image-compatibility.spec.v1alpha1+yaml\", \"digest\": \"sha256:4a47f8ae4c713906618413cb9795824d09eeadf948729e213a1ba11a1e31d052\", \"size\": 1710 } ], \"subject\": { \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\", \"digest\": \"sha256:5b0bcabd1ed22e9fb1310cf6c2dec7cdef19f0ad69efa1f392e94a4333501270\", \"size\": 7682 }, \"annotations\": { \"oci.opencontainers.image.created\": \"2024-03-27T08:08:08Z\" } } . Attach the artifact to the image . Create an image compatibility specification for the image, then install the ORAS tool and execute oras attach command. Example: . oras attach --artifact-type application/vnd.nfd.image-compatibility.v1alpha1 \\ &lt;image-url&gt; &lt;path-to-spec&gt;.yaml:application/vnd.nfd.image-compatibility.spec.v1alpha1+yaml . Note: The attach command is planned to be integrated into the nfd client tool. This will streamline the process, allowing you to perform the operation directly within the tool rather than using a separate command. Validate the host against the image compatibility specification . | Build nfd client: make build | Run ./bin/nfd compat validate-node --image &lt;image-url&gt; | . For more information about the available commands and flags, refer to the client documentation. Validate the k8s cluster node with the validate-image Job . Note: This does not require installation of NFD master and workers. Additionally, public registry certificates must be included in the job. In the example below, this is done using hostPath, but it can be done using any Kubernetes-supported method. apiVersion: batch/v1 kind: Job metadata: name: validate-image spec: backoffLimit: 1 template: spec: restartPolicy: Never containers: - name: image-compatibility securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true runAsNonRoot: true image: &lt;image-with-nfd-client&gt; command: [\"nfd\", \"compat\", \"validate-node\", \"--image\", \"&lt;image-to-be-validated&gt;\"] volumeMounts: - mountPath: /host-boot name: host-boot readOnly: true - mountPath: /host-etc/os-release name: host-os-release readOnly: true - mountPath: /host-sys name: host-sys readOnly: true - mountPath: /host-usr/lib name: host-usr-lib readOnly: true - mountPath: /host-lib name: host-lib readOnly: true - mountPath: /host-proc name: host-proc readOnly: true volumes: - hostPath: path: /boot type: \"\" name: host-boot - hostPath: path: /etc/os-release type: \"\" name: host-os-release - hostPath: path: /sys type: \"\" name: host-sys - hostPath: path: /usr/lib type: \"\" name: host-usr-lib - hostPath: path: /lib type: \"\" name: host-lib - hostPath: path: /proc type: \"\" name: host-proc - hostPath: path: \"&lt;path-to-registry-public-certs&gt;\" type: \"\" name: certs . ",
    "url": "/node-feature-discovery/master/usage/image-compatibility.html#image-compatibility",
    
    "relUrl": "/usage/image-compatibility.html#image-compatibility"
  },"44": {
    "doc": "Image variants",
    "title": "Image variants",
    "content": ". NFD offers two variants of the container image. Released container images are available for x86_64 and Arm64 architectures. ",
    "url": "/node-feature-discovery/master/deployment/image-variants.html",
    
    "relUrl": "/deployment/image-variants.html"
  },"45": {
    "doc": "Image variants",
    "title": "Default",
    "content": "The default is a minimal image based on scratch and only supports running statically linked binaries. For backwards compatibility a container image tag with suffix -minimal (e.g. gcr.io/k8s-staging-nfd/node-feature-discovery:master-minimal) is provided. ",
    "url": "/node-feature-discovery/master/deployment/image-variants.html#default",
    
    "relUrl": "/deployment/image-variants.html#default"
  },"46": {
    "doc": "Image variants",
    "title": "Full",
    "content": "This image is based on debian:bookworm-slim and contains a full Linux system for doing live debugging and diagnosis of the NFD images. The container image tag has suffix -full (e.g. gcr.io/k8s-staging-nfd/node-feature-discovery:master-full). ",
    "url": "/node-feature-discovery/master/deployment/image-variants.html#full",
    
    "relUrl": "/deployment/image-variants.html#full"
  },"47": {
    "doc": "Contributing",
    "title": "Contributing",
    "content": ". ",
    "url": "/node-feature-discovery/master/contributing/",
    
    "relUrl": "/contributing/"
  },"48": {
    "doc": "Contributing",
    "title": "Community",
    "content": "You can reach us via the following channels: . | #node-feature-discovery channel in Kubernetes Slack | SIG-Node mailing list | File an issue in this repository | . ",
    "url": "/node-feature-discovery/master/contributing/#community",
    
    "relUrl": "/contributing/#community"
  },"49": {
    "doc": "Contributing",
    "title": "Governance",
    "content": "This is a SIG-node subproject, hosted under the Kubernetes SIGs organization in Github. The project was established in 2016 and was migrated to Kubernetes SIGs in 2018. ",
    "url": "/node-feature-discovery/master/contributing/#governance",
    
    "relUrl": "/contributing/#governance"
  },"50": {
    "doc": "Contributing",
    "title": "License",
    "content": "This is open source software released under the Apache 2.0 License. ",
    "url": "/node-feature-discovery/master/contributing/#license",
    
    "relUrl": "/contributing/#license"
  },"51": {
    "doc": "Usage",
    "title": "Usage",
    "content": "Usage instructions. ",
    "url": "/node-feature-discovery/master/usage/",
    
    "relUrl": "/usage/"
  },"52": {
    "doc": "Developer guide",
    "title": "Developer guide",
    "content": " ",
    "url": "/node-feature-discovery/master/developer-guide/",
    
    "relUrl": "/developer-guide/"
  },"53": {
    "doc": "Developer guide",
    "title": "Table of contents",
    "content": ". | Building from source . | Download the source code | Docker build | Docker multi-arch builds with buildx | Deployment | Building locally | Customizing the build | Testing | NFD-Master | NFD-Worker | NFD-Topology-Updater | . | Running with Tilt . | Prerequisites | Environment variables | . | Documentation | . ",
    "url": "/node-feature-discovery/master/developer-guide/#table-of-contents",
    
    "relUrl": "/developer-guide/#table-of-contents"
  },"54": {
    "doc": "Developer guide",
    "title": "Building from source",
    "content": "Download the source code . git clone https://github.com/kubernetes-sigs/node-feature-discovery cd node-feature-discovery . Docker build . Build the container image . See customizing the build below for altering the container image registry, for example. make . Push the container image . Optional, this example with Docker. docker push &lt;IMAGE_TAG&gt; . Docker multi-arch builds with buildx . The default set of architectures enabled for mulit-arch builds are linux/amd64 and linux/arm64. If more architectures are needed one can override the IMAGE_ALL_PLATFORMS variable with a comma separated list of OS/ARCH tuples. Build the manifest-list with a container image per arch . make image-all . Currently docker does not support loading of manifest-lists meaning the images are not shown when executing docker images, see: buildx issue #59. Push the manifest-list with container image per arch . make push-all . The resulting container image can be used in the same way on each arch by pulling e.g. node-feature-discovery:master without specifying the architecture. The manifest-list will take care of providing the right architecture image. Change the job spec to use your custom image (optional) . To use your published image from the step above instead of the registry.k8s.io/nfd/node-feature-discovery image, edit image attribute in the spec template(s) to the new location (&lt;registry-name&gt;/&lt;image-name&gt;[:&lt;version&gt;]). Deployment . The yamls makefile generates a kustomization.yaml matching your locally built image and using the deploy/overlays/default deployment. See build customization below for configurability, e.g. changing the deployment namespace. K8S_NAMESPACE=my-ns make yamls kubectl apply -k . You can use alternative deployment methods by modifying the auto-generated kustomization file. Building locally . You can also build the binaries locally . make build . This will compile binaries under bin/ . Customizing the build . There are several Makefile variables that control the build process and the name of the resulting container image. The following are targeted targeted for build customization and they can be specified via environment variables or makefile overrides. | Variable | Description | Default value | . | HOSTMOUNT_PREFIX | Prefix of system directories for feature discovery (local builds) | / (local builds) /host- (container builds) | . | IMAGE_BUILD_CMD | Command to build the image | docker build | . | IMAGE_BUILD_EXTRA_OPTS | Extra options to pass to build command | empty | . | IMAGE_BUILDX_CMD | Command to build and push multi-arch images with buildx | DOCKER_CLI_EXPERIMENTAL=enabled docker buildx build –platform=${IMAGE_ALL_PLATFORMS} –progress=auto –pull | . | IMAGE_ALL_PLATFORMS | Comma separated list of OS/ARCH tuples for mulit-arch builds | linux/amd64,linux/arm64 | . | IMAGE_PUSH_CMD | Command to push the image to remote registry | docker push | . | IMAGE_REGISTRY | Container image registry to use | registry.k8s.io/nfd | . | IMAGE_TAG_NAME | Container image tag name | &lt;nfd version&gt; | . | IMAGE_EXTRA_TAG_NAMES | Additional container image tag(s) to create when building image | empty | . | K8S_NAMESPACE | nfd-master and nfd-worker namespace | node-feature-discovery | . For example, to use a custom registry: . make IMAGE_REGISTRY=&lt;my custom registry uri&gt; . Or to specify a build tool different from Docker, It can be done in 2 ways: . | via environment . IMAGE_BUILD_CMD=\"buildah bud\" make . | by overriding the variable value . make IMAGE_BUILD_CMD=\"buildah bud\" . | . Testing . Unit tests are automatically run as part of the container image build. You can also run them manually in the source code tree by running: . make test . End-to-end tests are built on top of the e2e test framework of Kubernetes, and, they required a cluster to run them on. For running the tests on your test cluster you need to specify the kubeconfig to be used: . make e2e-test KUBECONFIG=$HOME/.kube/config . There are several environment variables that can be used to customize the e2e-tests: . | Variable | Description | Default value | . | KUBECONFIG | Kubeconfig for running e2e-tests | empty | . | E2E_TEST_CONFIG | Parameterization file of e2e-tests (see example) | empty | . | E2E_PULL_IF_NOT_PRESENT | True-ish value makes the image pull policy IfNotPresent (to be used only in e2e tests) | false | . | E2E_TEST_FULL_IMAGE | Run e2e-test also against the Full Image tag | false | . | E2E_GINKGO_LABEL_FILTER | Ginkgo label filter to use for running e2e tests | empty | . | OPENSHIFT | Non-empty value enables OpenShift specific support (only affects e2e tests) | empty | . NFD-Master . For development and debugging it is possible to run nfd-master as a stand-alone binary outside the cluster. The -no-publish flag can be used to prevent nfd-master making changes to the nodes. If -no-publish is not set, nfd-master also requires the NODE_NAME environment variable to be set for cleaning up stale annotations. make build NODE_NAME=&lt;EXISTING_NODE&gt; ./nfd-master -no-publish -kubeconfig ~/.kube/config . NFD-Worker . For development and debugging it is possible to run nfd-worker as a stand-alone binary outside the cluster. The -no-publish flag can be used to prevent nfd-worker from creating NodeFeature objects in the target cluster. If the -no-publish is not set, nfd-worker also requires the NODE_NAME and KUBERNETES_NAMESPACE environment variables to be defined to create the NodeFeature object in the target cluster. make build KUBERNETES_NAMESPACE=default NODE_NAME=nonexistent-node ./bin/nfd-worker -kubeconfig ~/.kube/config . NOTE: Running nfd-worker locally this way discovers and publishes features of the local development system you’re running nfd-worker on. NFD-Topology-Updater . For development and debugging it is possible to run nfd-topology-updater as a stand-alone binary outside the cluster. However, it requires access to the kubelet’s local pod-resources socket and the kubelet http api so in practice it needs to be run on a host acting as a Kubernetes node and thus running kubelet. Running kubelet with --read-only-port=10255 (or readOnlyPort: 10255 in config) makes it possible to connect to kubelet without auth-token (never do this in a production cluster). Also, the -no-publish flag can be used to prevent nfd-topology-updater from creating NodeResourceTopology objects in the target cluster. If the -no-publish is not set, nfd-topology-updater also requires the NODE_NAME and KUBERNETES_NAMESPACE environment variables to be defined. make build KUBERNETES_NAMESPACE=default NODE_NAME=nonexistent-node ./bin/nfd-topology-updater -kubeconfig ~/.kube/config -kubelet-config-uri http://127.0.0.1:10255 . ",
    "url": "/node-feature-discovery/master/developer-guide/#building-from-source",
    
    "relUrl": "/developer-guide/#building-from-source"
  },"55": {
    "doc": "Developer guide",
    "title": "Running with Tilt",
    "content": "Another option for building NFD locally is via Tilt tool, which can build container images, push them to a local registry and reload your Kubernetes pods automatically. When using Tilt, you don’t have to build container images and re-deploy your pods manually but instead let the Tilt take care of it. Tiltfile is a configuration file for the Tilt and is located at the root directory. To develop NFD with Tilt, follow the steps below. Prerequisites . | Install Docker | Setup Docker as a non-root user. | Install kubectl | Install kustomize | Install tilt | Create a local Kubernetes cluster . | Create image registry first | Create a Kubernetes cluster. Please note that docker containers will be served as controller node and worker nodes, and NFD-worker will run as a DaemonSet in nested container. Therefore, to make sure the NFD-worker can discover the host features, the host folders “/boot” and “/lib” should be mounted into worker node docker containers when creating the Kubernetes cluster. | . | Start up node feature discovery development environment To start up your Tilt development environment, run at the root of your local NFD codebase. tilt up . Tilt will start a web interface in the localhost and port 10350. From the web interface, you are able to see how NFD worker and master are progressing, watch their build and runtime logs. Once your code changes are saved locally, Tilt will notice it and re-build the container image from the current code, push the image to the registry and re-deploy NFD pods with the latest container image. | . Environment variables . To override environment variables used in the Tiltfile during image build, export them in your current terminal before starting Tilt. export IMAGE_TAG_NAME=\"v1\" tilt up . This will override the default value(master) of IMAGE_TAG_NAME variable defined in the Tiltfile. ",
    "url": "/node-feature-discovery/master/developer-guide/#running-with-tilt",
    
    "relUrl": "/developer-guide/#running-with-tilt"
  },"56": {
    "doc": "Developer guide",
    "title": "Documentation",
    "content": "All documentation resides under the docs directory in the source tree. It is designed to be served as a html site by GitHub Pages. Building the documentation is containerized to fix the build environment. The recommended way for developing documentation is to run: . make site-serve . This will build the documentation in a container and serve it under localhost:4000/ making it easy to verify the results. Any changes made to the docs/ will automatically re-trigger a rebuild and are reflected in the served content and can be inspected with a browser refresh. To just build the html documentation run: . make site-build . This will generate html documentation under docs/_site/. ",
    "url": "/node-feature-discovery/master/developer-guide/#documentation",
    
    "relUrl": "/developer-guide/#documentation"
  },"57": {
    "doc": "Deployment",
    "title": "Deployment",
    "content": "Node Feature Discovery can be deployed on any recent version of Kubernetes (v1.24+). See Image variants for description of the different NFD container images available. Using Kustomize provides straightforward deployment with kubectl integration and declarative customization. Using Helm provides easy management of NFD deployments with nice configuration management and easy upgrades. Using Operator provides deployment and configuration management via CRDs. ",
    "url": "/node-feature-discovery/master/deployment/",
    
    "relUrl": "/deployment/"
  },"58": {
    "doc": "Reference",
    "title": "Reference",
    "content": "Command line and configuration reference. ",
    "url": "/node-feature-discovery/master/reference/",
    
    "relUrl": "/reference/"
  },"59": {
    "doc": "Get started",
    "title": "Node Feature Discovery",
    "content": "Welcome to Node Feature Discovery – a Kubernetes add-on for detecting hardware features and system configuration! . Continue to: . | Introduction for more details on the project. | Quick start for quick step-by-step instructions on how to get NFD running on your cluster. | . ",
    "url": "/node-feature-discovery/master/get-started/#node-feature-discovery",
    
    "relUrl": "/get-started/#node-feature-discovery"
  },"60": {
    "doc": "Get started",
    "title": "Quick-start – the short-short version",
    "content": "$ kubectl apply -k https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/default?ref=master namespace/node-feature-discovery created serviceaccount/nfd-master created clusterrole.rbac.authorization.k8s.io/nfd-master created clusterrolebinding.rbac.authorization.k8s.io/nfd-master created configmap/nfd-worker-conf created deployment.apps/nfd-master created daemonset.apps/nfd-worker created $ kubectl -n node-feature-discovery get all NAME READY STATUS RESTARTS AGE pod/nfd-master-555458dbbc-sxg6w 1/1 Running 0 56s pod/nfd-worker-mjg9f 1/1 Running 0 17s ... $ kubectl get nodes -o json | jq '.items[].metadata.labels' { \"kubernetes.io/arch\": \"amd64\", \"kubernetes.io/os\": \"linux\", \"feature.node.kubernetes.io/cpu-cpuid.ADX\": \"true\", \"feature.node.kubernetes.io/cpu-cpuid.AESNI\": \"true\", ... ",
    "url": "/node-feature-discovery/master/get-started/#quick-start--the-short-short-version",
    
    "relUrl": "/get-started/#quick-start--the-short-short-version"
  },"61": {
    "doc": "Get started",
    "title": "Get started",
    "content": " ",
    "url": "/node-feature-discovery/master/get-started/",
    
    "relUrl": "/get-started/"
  },"62": {
    "doc": "Introduction",
    "title": "Introduction",
    "content": " ",
    "url": "/node-feature-discovery/master/get-started/introduction.html",
    
    "relUrl": "/get-started/introduction.html"
  },"63": {
    "doc": "Introduction",
    "title": "Table of contents",
    "content": ". | NFD-Master | NFD-Worker | NFD-Topology-Updater | NFD-GC | Feature Discovery | Node annotations | Custom resources | . This software enables node feature discovery for Kubernetes. It detects hardware features available on each node in a Kubernetes cluster, and advertises those features using node labels and optionally node extended resources, annotations and node taints. Node Feature Discovery is compatible with any recent version of Kubernetes (v1.24+). NFD consists of four software components: . | nfd-master | nfd-worker | nfd-topology-updater | nfd-gc | . ",
    "url": "/node-feature-discovery/master/get-started/introduction.html#table-of-contents",
    
    "relUrl": "/get-started/introduction.html#table-of-contents"
  },"64": {
    "doc": "Introduction",
    "title": "NFD-Master",
    "content": "NFD-Master is the daemon responsible for communication towards the Kubernetes API. That is, it receives labeling requests from the worker and modifies node objects accordingly. ",
    "url": "/node-feature-discovery/master/get-started/introduction.html#nfd-master",
    
    "relUrl": "/get-started/introduction.html#nfd-master"
  },"65": {
    "doc": "Introduction",
    "title": "NFD-Worker",
    "content": "NFD-Worker is a daemon responsible for feature detection. It then communicates the information to nfd-master which does the actual node labeling. One instance of nfd-worker is supposed to be running on each node of the cluster, . ",
    "url": "/node-feature-discovery/master/get-started/introduction.html#nfd-worker",
    
    "relUrl": "/get-started/introduction.html#nfd-worker"
  },"66": {
    "doc": "Introduction",
    "title": "NFD-Topology-Updater",
    "content": "NFD-Topology-Updater is a daemon responsible for examining allocated resources on a worker node to account for resources available to be allocated to new pod on a per-zone basis (where a zone can be a NUMA node). It then creates or updates a NodeResourceTopology custom resource object specific to this node. One instance of nfd-topology-updater is supposed to be running on each node of the cluster. ",
    "url": "/node-feature-discovery/master/get-started/introduction.html#nfd-topology-updater",
    
    "relUrl": "/get-started/introduction.html#nfd-topology-updater"
  },"67": {
    "doc": "Introduction",
    "title": "NFD-GC",
    "content": "NFD-GC is a daemon responsible for cleaning obsolete NodeFeature and NodeResourceTopology objects. One instance of nfd-gc is supposed to be running in the cluster. ",
    "url": "/node-feature-discovery/master/get-started/introduction.html#nfd-gc",
    
    "relUrl": "/get-started/introduction.html#nfd-gc"
  },"68": {
    "doc": "Introduction",
    "title": "Feature Discovery",
    "content": "Feature discovery is divided into domain-specific feature sources: . | CPU | Kernel | Memory | Network | PCI | Storage | System | USB | Custom (rule-based custom features) | Local (features files) | . Each feature source is responsible for detecting a set of features which. in turn, are turned into node feature labels. Feature labels are prefixed with feature.node.kubernetes.io/ and also contain the name of the feature source. Non-standard user-specific feature labels can be created with the local and custom feature sources. An overview of the default feature labels: . { \"feature.node.kubernetes.io/cpu-&lt;feature-name&gt;\": \"true\", \"feature.node.kubernetes.io/custom-&lt;feature-name&gt;\": \"true\", \"feature.node.kubernetes.io/kernel-&lt;feature name&gt;\": \"&lt;feature value&gt;\", \"feature.node.kubernetes.io/memory-&lt;feature-name&gt;\": \"true\", \"feature.node.kubernetes.io/network-&lt;feature-name&gt;\": \"true\", \"feature.node.kubernetes.io/pci-&lt;device label&gt;.present\": \"true\", \"feature.node.kubernetes.io/storage-&lt;feature-name&gt;\": \"true\", \"feature.node.kubernetes.io/system-&lt;feature name&gt;\": \"&lt;feature value&gt;\", \"feature.node.kubernetes.io/usb-&lt;device label&gt;.present\": \"&lt;feature value&gt;\", \"feature.node.kubernetes.io/&lt;file name&gt;-&lt;feature name&gt;\": \"&lt;feature value&gt;\" } . ",
    "url": "/node-feature-discovery/master/get-started/introduction.html#feature-discovery",
    
    "relUrl": "/get-started/introduction.html#feature-discovery"
  },"69": {
    "doc": "Introduction",
    "title": "Node annotations",
    "content": "NFD also annotates nodes it is running on: . | Annotation | Description | . | [&lt;instance&gt;.]nfd.node.kubernetes.io/feature-labels | Comma-separated list of node labels managed by NFD. NFD uses this internally so must not be edited by users. | . | [&lt;instance&gt;.]nfd.node.kubernetes.io/feature-annotations | Comma-separated list of node annotations managed by NFD. NFD uses this internally so must not be edited by users. | . | [&lt;instance&gt;.]nfd.node.kubernetes.io/extended-resources | Comma-separated list of node extended resources managed by NFD. NFD uses this internally so must not be edited by users. | . | [&lt;instance&gt;.]nfd.node.kubernetes.io/taints | Comma-separated list of node taints managed by NFD. NFD uses this internally so must not be edited by users. | . NOTE: the -instance command line flag affects the annotation names . Unapplicable annotations are not created, i.e. for example nfd.node.kubernetes.io/extended-resources is only placed if some extended resources were created by NFD. ",
    "url": "/node-feature-discovery/master/get-started/introduction.html#node-annotations",
    
    "relUrl": "/get-started/introduction.html#node-annotations"
  },"70": {
    "doc": "Introduction",
    "title": "Custom resources",
    "content": "NFD takes use of some Kubernetes Custom Resources. NodeFeatures is be used for representing node features and requesting node labels to be generated. NFD-Master uses NodeFeatureRules for custom labeling of nodes. NFD-Topology-Updater creates NodeResourceTopology objects that describe the hardware topology of node resources. ",
    "url": "/node-feature-discovery/master/get-started/introduction.html#custom-resources",
    
    "relUrl": "/get-started/introduction.html#custom-resources"
  },"71": {
    "doc": "Kubectl plugin",
    "title": "Kubectl plugin",
    "content": " ",
    "url": "/node-feature-discovery/master/usage/kubectl-plugin.html",
    
    "relUrl": "/usage/kubectl-plugin.html"
  },"72": {
    "doc": "Kubectl plugin",
    "title": "Table of contents",
    "content": ". | Overview . | Validate | Test | DryRun | . | . Developer Preview This feature is currently in developer preview and subject to change. It is not recommended to use it in production environments. ",
    "url": "/node-feature-discovery/master/usage/kubectl-plugin.html#table-of-contents",
    
    "relUrl": "/usage/kubectl-plugin.html#table-of-contents"
  },"73": {
    "doc": "Kubectl plugin",
    "title": "Overview",
    "content": "The kubectl plugin kubectl nfd can be used to validate/dryrun and test NodeFeatureRule objects. It can be installed with the following command: . git clone https://github.com/kubernetes-sigs/node-feature-discovery cd node-feature-discovery make build-kubectl-nfd KUBECTL_PATH=/usr/local/bin/ mv ./bin/kubectl-nfd ${KUBECTL_PATH} . Validate . The plugin can be used to validate a NodeFeatureRule object: . kubectl nfd validate -f &lt;nodefeaturerule.yaml&gt; . Test . The plugin can be used to test a NodeFeatureRule object against a node: . kubectl nfd test -f &lt;nodefeaturerule.yaml&gt; -n &lt;node-name&gt; . DryRun . The plugin can be used to DryRun a NodeFeatureRule object against a NodeFeature file: . kubectl get -n node-feature-discovery nodefeature &lt;nodename&gt; -o yaml &gt; &lt;nodefeature.yaml&gt; kubectl nfd dryrun -f &lt;nodefeaturerule.yaml&gt; -n &lt;nodefeature.yaml&gt; . Or you can use the example NodeFeature file(it is a minimal NodeFeature file): . $ kubectl nfd dryrun -f examples/nodefeaturerule.yaml -n examples/nodefeature.yaml Evaluating NodeFeatureRule \"examples/nodefeaturerule.yaml\" against NodeFeature \"examples/nodefeature.yaml\" Processing rule: my sample rule *** Labels *** vendor.io/my-sample-feature=true NodeFeatureRule \"examples/nodefeaturerule.yaml\" is valid for NodeFeature \"examples/nodefeature.yaml\" . ",
    "url": "/node-feature-discovery/master/usage/kubectl-plugin.html#overview",
    
    "relUrl": "/usage/kubectl-plugin.html#overview"
  },"74": {
    "doc": "Kustomize",
    "title": "Deployment with Kustomize",
    "content": " ",
    "url": "/node-feature-discovery/master/deployment/kustomize.html#deployment-with-kustomize",
    
    "relUrl": "/deployment/kustomize.html#deployment-with-kustomize"
  },"75": {
    "doc": "Kustomize",
    "title": "Table of contents",
    "content": ". | Overlays . | Worker one-shot | Master Worker Topologyupdater | Topologyupdater | Metrics | . | Uninstallation | . Kustomize can be used to deploy NFD. Customization of the deployment is done by maintaining declarative overlays on top of the base overlays in NFD. To follow the deployment instructions here, kubectl v1.24 or later is required. The kustomize overlays provided in the repo can be used directly: . kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/default?ref=master\" . This will required RBAC rules and deploy nfd-master (as a deployment) and nfd-worker (as daemonset) in the node-feature-discovery namespace. NOTE: nfd-topology-updater is not deployed as part of the default overlay. Refer to the Master Worker Topologyupdater and Topologyupdater below. Alternatively you can clone the repository and customize the deployment by creating your own overlays. See kustomize for more information about managing deployment configurations. ",
    "url": "/node-feature-discovery/master/deployment/kustomize.html#table-of-contents",
    
    "relUrl": "/deployment/kustomize.html#table-of-contents"
  },"76": {
    "doc": "Kustomize",
    "title": "Overlays",
    "content": "The NFD repository hosts a set of overlays for different usages and deployment scenarios under deployment/overlays . | default: default deployment of nfd-worker as a daemonset, described above | default-job: see Worker one-shot below | master-worker-topologyupdater: see Master Worker Topologyupdater below | topologyupdater: see Topology Updater below | prometheus: see Metrics below | prune: clean up the cluster after uninstallation, see Removing feature labels | samples/custom-rules: an example for spicing up the default deployment with a separately managed configmap of custom labeling rules, see Custom feature source for more information about custom node labels | . Worker one-shot . Feature discovery can alternatively be configured as a one-shot job. The default-job overlay may be used to achieve this: . NUM_NODES=$(kubectl get no -o jsonpath='{.items[*].metadata.name}' | wc -w) kubectl kustomize \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/default-job?ref=master\" | \\ sed s\"/NUM_NODES/$NUM_NODES/\" | \\ kubectl apply -f - . The example above launches as many jobs as there are non-master nodes. Note that this approach does not guarantee running once on every node. For example, tainted, non-ready nodes or some other reasons in Job scheduling may cause some node(s) will run extra job instance(s) to satisfy the request. Master Worker Topologyupdater . NFD-Master, nfd-worker and nfd-topology-updater can be configured to be deployed as separate pods. The master-worker-topologyupdater overlay may be used to achieve this: . kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/master-worker-topologyupdater?ref=master\" . Topologyupdater . To deploy just nfd-topology-updater (without nfd-master and nfd-worker) use the topologyupdater overlay: . kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/topologyupdater?ref=master\" . NFD-Topology-Updater can be configured along with the default overlay (which deploys nfd-worker and nfd-master) where all the software components are deployed as separate pods; . kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/default?ref=master\" kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/topologyupdater?ref=master\" . Metrics . To allow prometheus operator to scrape metrics from node-feature-discovery, run the following command: . kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/default?ref=master\" kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/prometheus?ref=master\" . ",
    "url": "/node-feature-discovery/master/deployment/kustomize.html#overlays",
    
    "relUrl": "/deployment/kustomize.html#overlays"
  },"77": {
    "doc": "Kustomize",
    "title": "Uninstallation",
    "content": "Simplest way is to invoke kubectl delete on the overlay that was used for deployment. Beware that this will also delete the namespace that NFD is running in. For example, in case the default overlay from the repo was used: . kubectl delete -k https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/default?ref=master . Alternatively you can delete create objects one-by-one, depending on the type of deployment, for example: . NFD_NS=node-feature-discovery kubectl -n $NFD_NS delete ds nfd-worker kubectl -n $NFD_NS delete deploy nfd-master kubectl -n $NFD_NS delete svc nfd-master kubectl -n $NFD_NS delete sa nfd-master kubectl delete clusterrole nfd-master kubectl delete clusterrolebinding nfd-master . ",
    "url": "/node-feature-discovery/master/deployment/kustomize.html#uninstallation",
    
    "relUrl": "/deployment/kustomize.html#uninstallation"
  },"78": {
    "doc": "Kustomize",
    "title": "Kustomize",
    "content": " ",
    "url": "/node-feature-discovery/master/deployment/kustomize.html",
    
    "relUrl": "/deployment/kustomize.html"
  },"79": {
    "doc": "Master cmdline reference",
    "title": "Commandline flags of nfd-master",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/master-commandline-reference.html#commandline-flags-of-nfd-master",
    
    "relUrl": "/reference/master-commandline-reference.html#commandline-flags-of-nfd-master"
  },"80": {
    "doc": "Master cmdline reference",
    "title": "Table of contents",
    "content": ". | -h, -help | -version | -feature-gates | -prune | -port | -instance | -enable-leader-election | -enable-taints | -no-publish | -label-whitelist | -extra-label-ns | -deny-label-ns | -informer-page-size | -config | -options | -nfd-api-parallelism | Logging | -resync-period | . To quickly view available command line flags execute nfd-master -help. In a docker container: . docker run gcr.io/k8s-staging-nfd/node-feature-discovery:master nfd-master -help . -h, -help . Print usage and exit. -version . Print version and exit. -feature-gates . The -feature-gates flag is used to enable or disable non GA features. The list of available feature gates can be found in the feature gates documentation. Example: . nfd-master -feature-gates NodeFeatureGroupAPI=true . -prune . The -prune flag is a sub-command like option for cleaning up the cluster. It causes nfd-master to remove all NFD related labels, annotations and extended resources from all Node objects of the cluster and exit. -port . The -port flag specifies the port on which metrics and healthz endpoints are served on. Default: 8080 . Example: . nfd-master -port=12345 . -instance . The -instance flag makes it possible to run multiple NFD deployments in parallel. In practice, it separates the node annotations between deployments so that each of them can store metadata independently. The instance name must start and end with an alphanumeric character and may only contain alphanumeric characters, -, _ or .. Default: empty . Example: . nfd-master -instance=network . -enable-leader-election . The -enable-leader-election flag enables leader election for NFD-Master. It is advised to turn on this flag when running more than one instance of NFD-Master. Default: false . nfd-master -enable-leader-election . -enable-taints . The -enable-taints flag enables/disables node tainting feature of NFD. Default: false . Example: . nfd-master -enable-taints=true . -no-publish . The -no-publish flag disables updates to the Node objects in the Kubernetes API server, making a “dry-run” flag for nfd-master. No Labels, Annotations or ExtendedResources of nodes are updated. Default: false . Example: . nfd-master -no-publish . -label-whitelist . The -label-whitelist specifies a regular expression for filtering feature labels based on their name. Each label must match against the given regular expression or it will not be published. NOTE: The regular expression is only matches against the “basename” part of the label, i.e. to the part of the name after ‘/’. The label namespace is omitted. Default: empty . Example: . nfd-master -label-whitelist='.*cpuid\\.' . -extra-label-ns . The -extra-label-ns flag specifies a comma-separated list of allowed feature label namespaces. This option can be used to allow other vendor or application specific namespaces for custom labels from the local and custom feature sources, even though these labels were denied using the deny-label-ns flag. Default: empty . Example: . nfd-master -extra-label-ns=vendor-1.com,vendor-2.io . -deny-label-ns . The -deny-label-ns flag specifies a comma-separated list of excluded label namespaces. By default, nfd-master allows creating labels in all namespaces, excluding kubernetes.io namespace and its sub-namespaces (i.e. *.kubernetes.io). However, you should note that kubernetes.io and its sub-namespaces are always denied. For example, nfd-master -deny-label-ns=\"\" would still disallow kubernetes.io and *.kubernetes.io. This option can be used to exclude some vendors or application specific namespaces. Note that the namespaces feature.node.kubernetes.io and profile.node.kubernetes.io and their sub-namespaces are always allowed and cannot be denied. Default: empty . Example: . nfd-master -deny-label-ns=*.vendor.com,vendor-2.io . -informer-page-size . The -informer-page-size flag is used to control pagination during informer cache sync on nfd-master startup. This is useful to control load on api-server/etcd as listing NodeFeature objects can be expensive, especially in large clusters. Default: 200 . Example: . nfd-master -informer-page-size=20 . -config . The -config flag specifies the path of the nfd-master configuration file to use. Default: /etc/kubernetes/node-feature-discovery/nfd-master.conf . Example: . nfd-master -config=/opt/nfd/master.conf . -options . The -options flag may be used to specify and override configuration file options directly from the command line. The required format is the same as in the config file i.e. JSON or YAML. Configuration options specified via this flag will override those from the configuration file: . Default: empty . Example: . nfd-master -options='{\"noPublish\": true}' . -nfd-api-parallelism . The -nfd-api-parallelism flag can be used to specify the maximum number of concurrent node updates. Default: 10 . Example: . nfd-master -nfd-api-parallelism=1 . Logging . The following logging-related flags are inherited from the klog package. -add_dir_header . If true, adds the file directory to the header of the log messages. Default: false . -alsologtostderr . Log to standard error as well as files. Default: false . -log_backtrace_at . When logging hits line file:N, emit a stack trace. Default: empty . -log_dir . If non-empty, write log files in this directory. Default: empty . -log_file . If non-empty, use this log file. Default: empty . -log_file_max_size . Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. Default: 1800 . -logtostderr . Log to standard error instead of files . Default: true . -skip_headers . If true, avoid header prefixes in the log messages. Default: false . -skip_log_headers . If true, avoid headers when opening log files. Default: false . -stderrthreshold . Logs at or above this threshold go to stderr. Default: 2 . -v . Number for the log level verbosity. Default: 0 . -vmodule . Comma-separated list of pattern=N settings for file-filtered logging. Default: empty . -resync-period . The -resync-period flag specifies the NFD API controller resync period. The resync means nfd-master replaying all NodeFeature and NodeFeatureRule objects, thus effectively re-syncing all nodes in the cluster (i.e. ensuring labels, annotations, extended resources and taints are in place). Default: 1 hour. Example: . nfd-master -resync-period=2h . ",
    "url": "/node-feature-discovery/master/reference/master-commandline-reference.html#table-of-contents",
    
    "relUrl": "/reference/master-commandline-reference.html#table-of-contents"
  },"81": {
    "doc": "Master cmdline reference",
    "title": "Master cmdline reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/master-commandline-reference.html",
    
    "relUrl": "/reference/master-commandline-reference.html"
  },"82": {
    "doc": "Master config reference",
    "title": "Configuration file reference of nfd-master",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#configuration-file-reference-of-nfd-master",
    
    "relUrl": "/reference/master-configuration-reference.html#configuration-file-reference-of-nfd-master"
  },"83": {
    "doc": "Master config reference",
    "title": "Table of contents",
    "content": ". | noPublish | extraLabelNs | denyLabelNs | enableTaints | labelWhiteList | resyncPeriod | leaderElection . | leaderElection.leaseDuration | leaderElection.renewDeadline | leaderElection.retryPeriod | . | nfdApiParallelism | informerPageSize | klog . | klog.addDirHeader | klog.alsologtostderr | klog.logBacktraceAt | klog.logDir | klog.logFile | klog.logFileMaxSize | klog.logtostderr | klog.skipHeaders | klog.skipLogHeaders | klog.stderrthreshold | klog.v | klog.vmodule | . | restrictions (EXPERIMENTAL) . | restrictions.nodeFeatureNamespaceSelector | restrictions.disableLabels | restrictions.disableExtendedResources | restrictions.disableAnnotations | restrictions.allowOverwrite | restrictions.denyNodeFeatureLabels | . | . See the sample configuration file for a full example configuration. ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#table-of-contents",
    
    "relUrl": "/reference/master-configuration-reference.html#table-of-contents"
  },"84": {
    "doc": "Master config reference",
    "title": "noPublish",
    "content": "noPublish option disables updates to the Node objects in the Kubernetes API server, making a “dry-run” flag for nfd-master. No Labels, Annotations, Taints or ExtendedResources of nodes are updated. Default: false . Example: . noPublish: true . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#nopublish",
    
    "relUrl": "/reference/master-configuration-reference.html#nopublish"
  },"85": {
    "doc": "Master config reference",
    "title": "extraLabelNs",
    "content": "extraLabelNs specifies a list of allowed feature label namespaces. This option can be used to allow other vendor or application specific namespaces for custom labels from the local and custom feature sources, even though these labels were denied using the denyLabelNs parameter. Default: empty . Example: . extraLabelNs: [\"added.ns.io\",\"added.kubernets.io\"] . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#extralabelns",
    
    "relUrl": "/reference/master-configuration-reference.html#extralabelns"
  },"86": {
    "doc": "Master config reference",
    "title": "denyLabelNs",
    "content": "denyLabelNs specifies a list of excluded label namespaces. By default, nfd-master allows creating labels in all namespaces, excluding kubernetes.io namespace and its sub-namespaces (i.e. *.kubernetes.io). However, you should note that kubernetes.io and its sub-namespaces are always denied. This option can be used to exclude some vendors or application specific namespaces. Default: empty . Example: . denyLabelNs: [\"denied.ns.io\",\"denied.kubernetes.io\"] . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#denylabelns",
    
    "relUrl": "/reference/master-configuration-reference.html#denylabelns"
  },"87": {
    "doc": "Master config reference",
    "title": "enableTaints",
    "content": "enableTaints enables/disables node tainting feature of NFD. Default: false . Example: . enableTaints: true . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#enabletaints",
    
    "relUrl": "/reference/master-configuration-reference.html#enabletaints"
  },"88": {
    "doc": "Master config reference",
    "title": "labelWhiteList",
    "content": "labelWhiteList specifies a regular expression for filtering feature labels based on their name. Each label must match against the given regular expression or it will not be published. ** NOTE:** The regular expression is only matches against the “basename” part of the label, i.e. to the part of the name after ‘/’. The label namespace is omitted. Default: empty . Example: . labelWhiteList: \"foo\" . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#labelwhitelist",
    
    "relUrl": "/reference/master-configuration-reference.html#labelwhitelist"
  },"89": {
    "doc": "Master config reference",
    "title": "resyncPeriod",
    "content": "The resyncPeriod option specifies the NFD API controller resync period. The resync means nfd-master replaying all NodeFeature and NodeFeatureRule objects, thus effectively re-syncing all nodes in the cluster (i.e. ensuring labels, annotations, extended resources and taints are in place). Default: 1 hour. Example: . resyncPeriod: 2h . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#resyncperiod",
    
    "relUrl": "/reference/master-configuration-reference.html#resyncperiod"
  },"90": {
    "doc": "Master config reference",
    "title": "leaderElection",
    "content": "The leaderElection section exposes configuration to tweak leader election. leaderElection.leaseDuration . leaderElection.leaseDuration is the duration that non-leader candidates will wait to force acquire leadership. This is measured against time of last observed ack. A client needs to wait a full LeaseDuration without observing a change to the record before it can attempt to take over. When all clients are shutdown and a new set of clients are started with different names against the same leader record, they must wait the full LeaseDuration before attempting to acquire the lease. Thus LeaseDuration should be as short as possible (within your tolerance for clock skew rate) to avoid a possible long waits in the scenario. Default: 15 seconds. Example: . leaderElection: leaseDurtation: 15s . leaderElection.renewDeadline . leaderElection.renewDeadline is the duration that the acting master will retry refreshing leadership before giving up. This value has to be lower than leaseDuration and greater than retryPeriod*1.2. Default: 10 seconds. Example: . leaderElection: renewDeadline: 10s . leaderElection.retryPeriod . leaderElection.retryPeriod is the duration the LeaderElector clients should wait between tries of actions. It has to be greater than 0. Default: 2 seconds. Example: . leaderElection: retryPeriod: 2s . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#leaderelection",
    
    "relUrl": "/reference/master-configuration-reference.html#leaderelection"
  },"91": {
    "doc": "Master config reference",
    "title": "nfdApiParallelism",
    "content": "The nfdApiParallelism option can be used to specify the maximum number of concurrent node updates. Default: 10 . Example: . nfdApiParallelism: 1 . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#nfdapiparallelism",
    
    "relUrl": "/reference/master-configuration-reference.html#nfdapiparallelism"
  },"92": {
    "doc": "Master config reference",
    "title": "informerPageSize",
    "content": "The informerPageSize option is used to control pagination during informer cache sync on nfd-master startup. This is useful to control load on api-server/etcd as listing NodeFeature objects can be expensive, especially in large clusters. Default: 200 . Example: . informerPageSize: 50 . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#informerpagesize",
    
    "relUrl": "/reference/master-configuration-reference.html#informerpagesize"
  },"93": {
    "doc": "Master config reference",
    "title": "klog",
    "content": "The following options specify the logger configuration. Most of which can be dynamically adjusted at run-time. NOTE: The logger options can also be specified via command line flags which take precedence over any corresponding config file options. klog.addDirHeader . If true, adds the file directory to the header of the log messages. Default: false . Run-time configurable: yes . klog.alsologtostderr . Log to standard error as well as files. Default: false . Run-time configurable: yes . klog.logBacktraceAt . When logging hits line file:N, emit a stack trace. Default: empty . Run-time configurable: yes . klog.logDir . If non-empty, write log files in this directory. Default: empty . Run-time configurable: no . klog.logFile . If non-empty, use this log file. Default: empty . Run-time configurable: no . klog.logFileMaxSize . Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. Default: 1800 . Run-time configurable: no . klog.logtostderr . Log to standard error instead of files . Default: true . Run-time configurable: yes . klog.skipHeaders . If true, avoid header prefixes in the log messages. Default: false . Run-time configurable: yes . klog.skipLogHeaders . If true, avoid headers when opening log files. Default: false . Run-time configurable: no . klog.stderrthreshold . Logs at or above this threshold go to stderr (default 2) . Run-time configurable: yes . klog.v . Number for the log level verbosity. Default: 0 . Run-time configurable: yes . klog.vmodule . Comma-separated list of pattern=N settings for file-filtered logging. Default: empty . Run-time configurable: yes . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#klog",
    
    "relUrl": "/reference/master-configuration-reference.html#klog"
  },"94": {
    "doc": "Master config reference",
    "title": "restrictions (EXPERIMENTAL)",
    "content": "The following options specify the restrictions that can be applied by the nfd-master on the deployed Custom Resources in the cluster. restrictions.nodeFeatureNamespaceSelector . The nodeFeatureNamespaceSelector option specifies the NodeFeatures namespaces to watch, which can be selected by using metav1.LabelSelector as a type for this option. An empty value selects all namespaces to be watched. Default: empty . Example: . restrictions: nodeFeatureNamespaceSelector: matchLabels: kubernetes.io/metadata.name: \"node-feature-discovery\" matchExpressions: - key: \"kubernetes.io/metadata.name\" operator: \"In\" values: - \"node-feature-discovery\" . restrictions.disableLabels . The disableLabels option controls whether to allow creation of node labels from NodeFeature and NodeFeatureRule CRs or not. Default: false . Example: . restrictions: disableLabels: true . restrictions.disableExtendedResources . The disableExtendedResources option controls whether to allow creation of node extended resources from NodeFeatureRule CR or not. Default: false . Example: . restrictions: disableExtendedResources: true . restrictions.disableAnnotations . he disableAnnotations option controls whether to allow creation of node annotations from NodeFeatureRule CR or not. Default: false . Example: . restrictions: disableAnnotations: true . restrictions.allowOverwrite . The allowOverwrite option controls whether NFD is allowed to overwrite and take over management of existing node labels, annotations, and extended resources. Labels, annotations and extended resources created by NFD itself are not affected (overwrite cannot be disabled). NFD tracks the labels, annotations and extended resources that it manages with specific node annotations. Default: true . Example: . restrictions: allowOverwrite: false . restrictions.denyNodeFeatureLabels . The denyNodeFeatureLabels option specifies whether to deny labels from 3rd party NodeFeature objects or not. NodeFeature objects created by nfd-worker are not affected. Default: false . Example: . restrictions: denyNodeFeatureLabels: true . ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html#restrictions-experimental",
    
    "relUrl": "/reference/master-configuration-reference.html#restrictions-experimental"
  },"95": {
    "doc": "Master config reference",
    "title": "Master config reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/master-configuration-reference.html",
    
    "relUrl": "/reference/master-configuration-reference.html"
  },"96": {
    "doc": "Metrics",
    "title": "Metrics",
    "content": "Metrics are configured to be exposed using prometheus operator API’s by default. If you want to expose metrics using the prometheus operator API’s you need to install the prometheus operator in your cluster. By default NFD Master and Worker expose metrics on port 8081. The exposed metrics are . | Metric | Type | Description | . | nfd_master_build_info | Gauge | Version from which nfd-master was built | . | nfd_worker_build_info | Gauge | Version from which nfd-worker was built | . | nfd_gc_build_info | Gauge | Version from which nfd-gc was built | . | nfd_topology_updater_build_info | Gauge | Version from which nfd-topology-updater was built | . | nfd_master_node_update_requests_total | Counter | Number of node update requests received by the master over gRPC | . | nfd_master_node_updates_total | Counter | Number of nodes updated | . | nfd_master_node_feature_group_update_requests_total | Counter | Number of cluster feature update requests processed by the master | . | nfd_master_node_update_failures_total | Counter | Number of nodes update failures | . | nfd_master_node_labels_rejected_total | Counter | Number of nodes labels rejected by nfd-master | . | nfd_master_node_extendedresources_rejected_total | Counter | Number of nodes extended resources rejected by nfd-master | . | nfd_master_node_taints_rejected_total | Counter | Number of nodes taints rejected by nfd-master | . | nfd_master_nodefeaturerule_processing_duration_seconds | Histogram | Time taken to process NodeFeatureRule objects | . | nfd_master_nodefeaturerule_processing_errors_total | Counter | Number or errors encountered while processing NodeFeatureRule objects | . | nfd_worker_feature_discovery_duration_seconds | Histogram | Time taken to discover features on a node | . | nfd_topology_updater_scan_errors_total | Counter | Number of errors in scanning resource allocation of pods. | . | nfd_gc_objects_deleted_total | Counter | Number of NodeFeature and NodeResourceTopology objects garbage collected. | . | nfd_gc_object_delete_failures_total | Counter | Number of errors in deleting NodeFeature and NodeResourceTopology objects. | . ",
    "url": "/node-feature-discovery/master/deployment/metrics.html",
    
    "relUrl": "/deployment/metrics.html"
  },"97": {
    "doc": "Metrics",
    "title": "Kustomize",
    "content": "To deploy NFD with metrics enabled using kustomize, you can use the prometheus overlay. ",
    "url": "/node-feature-discovery/master/deployment/metrics.html#kustomize",
    
    "relUrl": "/deployment/metrics.html#kustomize"
  },"98": {
    "doc": "Metrics",
    "title": "Helm",
    "content": "By default metrics are enabled when deploying NFD via Helm. To enable Prometheus to scrape metrics from NFD, you need to pass the following values to Helm: . --set prometheus.enable=true . For more info on Helm deployment, see Helm. It is recommended to specify --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false when deploying prometheus-operator via Helm to enable the prometheus-operator to scrape metrics from any PodMonitor. or setting labels on the PodMonitor via the helm parameter prometheus.labels to control which Prometheus instances will scrape this PodMonitor. ",
    "url": "/node-feature-discovery/master/deployment/metrics.html#helm",
    
    "relUrl": "/deployment/metrics.html#helm"
  },"99": {
    "doc": "Metrics",
    "title": "Grafana dashboard",
    "content": "NFD contains an example Grafana dashboard. You can import examples/grafana-dashboard.json to your Grafana instance to visualize the NFD metrics. ",
    "url": "/node-feature-discovery/master/deployment/metrics.html#grafana-dashboard",
    
    "relUrl": "/deployment/metrics.html#grafana-dashboard"
  },"100": {
    "doc": "NFD-Garbage-Collector",
    "title": "NFD-GC",
    "content": ". NFD-GC (NFD Garbage-Collector) is preferably run as a Kubernetes deployment with one replica. It makes sure that all NodeFeature and NodeResourceTopology objects have corresponding nodes and removes stale objects for non-existent nodes. The daemon watches for Node deletion events and removes NodeFeature and NodeResourceTopology objects upon them. It also runs periodically to make sure no node delete event was missed and to remove any NodeFeature or NodeResourceTopology objects that were created without corresponding node. The default garbage collector interval is set to 1h which is the value when no -gc-interval is specified. ",
    "url": "/node-feature-discovery/master/usage/nfd-gc.html#nfd-gc",
    
    "relUrl": "/usage/nfd-gc.html#nfd-gc"
  },"101": {
    "doc": "NFD-Garbage-Collector",
    "title": "Configuration",
    "content": "In Helm deployments see garbage collector parameters for altering the nfd-gc configuration. ",
    "url": "/node-feature-discovery/master/usage/nfd-gc.html#configuration",
    
    "relUrl": "/usage/nfd-gc.html#configuration"
  },"102": {
    "doc": "NFD-Garbage-Collector",
    "title": "NFD-Garbage-Collector",
    "content": " ",
    "url": "/node-feature-discovery/master/usage/nfd-gc.html",
    
    "relUrl": "/usage/nfd-gc.html"
  },"103": {
    "doc": "NFD-Master",
    "title": "NFD-Master",
    "content": ". NFD-Master is responsible for connecting to the Kubernetes API server and updating node objects. More specifically, it modifies node labels, taints and extended resources based on requests from nfd-workers and 3rd party extensions. ",
    "url": "/node-feature-discovery/master/usage/nfd-master.html",
    
    "relUrl": "/usage/nfd-master.html"
  },"104": {
    "doc": "NFD-Master",
    "title": "NodeFeature controller",
    "content": "The NodeFeature Controller uses NodeFeature objects as the input for the NodeFeatureRule processing pipeline. In addition, any labels listed in the NodeFeature object are created on the node (note the allowed label namespaces are controlled). ",
    "url": "/node-feature-discovery/master/usage/nfd-master.html#nodefeature-controller",
    
    "relUrl": "/usage/nfd-master.html#nodefeature-controller"
  },"105": {
    "doc": "NFD-Master",
    "title": "NodeFeatureRule controller",
    "content": "NFD-Master acts as the controller for NodeFeatureRule objects. It applies the rules specified in NodeFeatureRule objects on raw feature data and creates node labels accordingly. The feature data used as the input is received from nfd-worker instances through NodeFeature objects. ",
    "url": "/node-feature-discovery/master/usage/nfd-master.html#nodefeaturerule-controller",
    
    "relUrl": "/usage/nfd-master.html#nodefeaturerule-controller"
  },"106": {
    "doc": "NFD-Master",
    "title": "Master configuration",
    "content": "NFD-Master supports configuration through a configuration file. The default location is /etc/kubernetes/node-feature-discovery/nfd-master.conf, but, this can be changed by specifying the-config command line flag. Master configuration file is read inside the container, and thus, Volumes and VolumeMounts are needed to make your configuration available for NFD. The preferred method is to use a ConfigMap which provides easy deployment and re-configurability. The provided deployment methods (Helm and Kustomize) create an empty configmap and mount it inside the nfd-master containers. In Helm deployments, Master pod parameter master.config can be used to edit the respective configuration. In Kustomize deployments, modify the nfd-master-conf ConfigMap with a custom overlay. NOTE: dynamic run-time reconfiguration was dropped in NFD v0.17. Re-configuration is handled by pod restarts. See nfd-master configuration file reference for more details. The (empty-by-default) example config contains all available configuration options and can be used as a reference for creating a configuration. ",
    "url": "/node-feature-discovery/master/usage/nfd-master.html#master-configuration",
    
    "relUrl": "/usage/nfd-master.html#master-configuration"
  },"107": {
    "doc": "NFD-Master",
    "title": "Deployment notes",
    "content": "NFD-Master runs as a deployment, by default it prefers running on the cluster’s master nodes but will run on worker nodes if no master nodes are found. For High Availability, you should increase the replica count of the deployment object. You should also look into adding inter-pod affinity to prevent masters from running on the same node. However note that inter-pod affinity is costly and is not recommended in bigger clusters. Note: When NFD-Master is intended to run with more than one replica, it is advised to use -enable-leader-election flag. This flag turns on leader election for NFD-Master and let only one replica to act on changes in NodeFeature and NodeFeatureRule objects. If you have RBAC authorization enabled (as is the default e.g. with clusters initialized with kubeadm) you need to configure the appropriate ClusterRoles, ClusterRoleBindings and a ServiceAccount for NFD to create node labels. The provided template will configure these for you. ",
    "url": "/node-feature-discovery/master/usage/nfd-master.html#deployment-notes",
    
    "relUrl": "/usage/nfd-master.html#deployment-notes"
  },"108": {
    "doc": "NFD-Master",
    "title": "Informer List Pagination",
    "content": "When NFD Master starts up it starts an informer on the nodefeatures resources. These resources can be large and in a large cluster this initial list call to sync the informer cache can be expensive and heavy on api-server/etcd. You can use the informer-list-size argument to NFD master to control pagination size to help control the load during NFD-Master restart. ",
    "url": "/node-feature-discovery/master/usage/nfd-master.html#informer-list-pagination",
    
    "relUrl": "/usage/nfd-master.html#informer-list-pagination"
  },"109": {
    "doc": "NFD-Topology-Updater",
    "title": "NFD-Topology-Updater",
    "content": ". NFD-Topology-Updater is preferably run as a Kubernetes DaemonSet. This assures re-examination on regular intervals and/or per pod life-cycle events, capturing changes in the allocated resources and hence the allocatable resources on a per-zone basis by updating NodeResourceTopology custom resources. It makes sure that new NodeResourceTopology instances are created for each new nodes that get added to the cluster. Because of the design and implementation of Kubernetes, only resources exclusively allocated to Guaranteed Quality of Service pods will be accounted. This includes CPU cores, memory and devices. When run as a daemonset, nodes are re-examined for the allocated resources (to determine the information of the allocatable resources on a per-zone basis where a zone can be a NUMA node) at an interval specified using the -sleep-interval option. The default sleep interval is set to 60s which is the value when no -sleep-interval is specified. The re-examination can be disabled by setting the sleep-interval to 0. Another option is to configure the updater to update the allocated resources per pod life-cycle events. The updater will monitor the checkpoint file stated in -kubelet-state-dir and triggers an update for every change occurs in the files. In addition, it can avoid examining specific allocated resources given a configuration of resources to exclude via -excludeList . ",
    "url": "/node-feature-discovery/master/usage/nfd-topology-updater.html",
    
    "relUrl": "/usage/nfd-topology-updater.html"
  },"110": {
    "doc": "NFD-Topology-Updater",
    "title": "Deployment Notes",
    "content": "Kubelet PodResource API with the GetAllocatableResources functionality enabled is a prerequisite for nfd-topology-updater to be able to run (i.e. Kubernetes v1.21 or later is required). Preceding Kubernetes v1.23, the kubelet must be started with --feature-gates=KubeletPodResourcesGetAllocatable=true. Starting from Kubernetes v1.23, the KubeletPodResourcesGetAllocatable feature gate. is enabled by default . ",
    "url": "/node-feature-discovery/master/usage/nfd-topology-updater.html#deployment-notes",
    
    "relUrl": "/usage/nfd-topology-updater.html#deployment-notes"
  },"111": {
    "doc": "NFD-Topology-Updater",
    "title": "Topology-Updater Configuration",
    "content": "NFD-Topology-Updater supports configuration through a configuration file. The default location is /etc/kubernetes/node-feature-discovery/topology-updater.conf, but, this can be changed by specifying the-config command line flag. Topology-Updater configuration file is read inside the container, and thus, Volumes and VolumeMounts are needed to make your configuration available for NFD. The preferred method is to use a ConfigMap which provides easy deployment and re-configurability. The provided deployment templates create an empty configmap and mount it inside the nfd-topology-updater containers. In Helm deployments, Topology Updater parameters toplogyUpdater.config can be used to edit the respective configuration. In Kustomize deployments, modify the nfd-worker-conf ConfigMap with a custom overlay. See nfd-topology-updater configuration file reference for more details. The (empty-by-default) example config contains all available configuration options and can be used as a reference for creating a configuration. ",
    "url": "/node-feature-discovery/master/usage/nfd-topology-updater.html#topology-updater-configuration",
    
    "relUrl": "/usage/nfd-topology-updater.html#topology-updater-configuration"
  },"112": {
    "doc": "NFD-Worker",
    "title": "NFD-Worker",
    "content": ". NFD-Worker is preferably run as a Kubernetes DaemonSet. This assures re-labeling on regular intervals capturing changes in the system configuration and makes sure that new nodes are labeled as they are added to the cluster. Worker connects to the nfd-master service to advertise hardware features. When run as a daemonset, nodes are re-labeled at an default interval of 60s. This can be changed by using the core.sleepInterval config option. ",
    "url": "/node-feature-discovery/master/usage/nfd-worker.html",
    
    "relUrl": "/usage/nfd-worker.html"
  },"113": {
    "doc": "NFD-Worker",
    "title": "Worker configuration",
    "content": "NFD-Worker supports configuration through a configuration file. The default location is /etc/kubernetes/node-feature-discovery/nfd-worker.conf, but, this can be changed by specifying the-config command line flag. Configuration file is re-read whenever it is modified which makes run-time re-configuration of nfd-worker straightforward. Worker configuration file is read inside the container, and thus, Volumes and VolumeMounts are needed to make your configuration available for NFD. The preferred method is to use a ConfigMap which provides easy deployment and re-configurability. The provided deployment methods (Helm and Kustomize) create an empty configmap and mount it inside the nfd-master containers. In Helm deployments, Worker pod parameter worker.config can be used to edit the respective configuration. In Kustomize deployments, modify the nfd-worker-conf ConfigMap with a custom overlay. NOTE: dynamic run-time reconfiguration was dropped in NFD v0.17. Re-configuration is handled by pod restarts. See nfd-worker configuration file reference for more details. The (empty-by-default) example config contains all available configuration options and can be used as a reference for creating a configuration. Configuration options can also be specified via the -options command line flag, in which case no mounts need to be used. The same format as in the config file must be used, i.e. JSON (or YAML). For example: . -options='{\"sources\": { \"pci\": { \"deviceClassWhitelist\": [\"12\"] } } }' . Configuration options specified from the command line will override those read from the config file. ",
    "url": "/node-feature-discovery/master/usage/nfd-worker.html#worker-configuration",
    
    "relUrl": "/usage/nfd-worker.html#worker-configuration"
  },"114": {
    "doc": "Node Feature client cmdline reference",
    "title": "Commandline flags of nfd client",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/node-feature-client-reference.html#commandline-flags-of-nfd-client",
    
    "relUrl": "/reference/node-feature-client-reference.html#commandline-flags-of-nfd-client"
  },"115": {
    "doc": "Node Feature client cmdline reference",
    "title": "Table of contents",
    "content": ". | -h, –help | compat . | validate-node | . | . The client is in the experimental v1alpha1 version. To quickly view available command line flags execute nfd --help. -h, –help . Print usage and exit. ",
    "url": "/node-feature-discovery/master/reference/node-feature-client-reference.html#table-of-contents",
    
    "relUrl": "/reference/node-feature-client-reference.html#table-of-contents"
  },"116": {
    "doc": "Node Feature client cmdline reference",
    "title": "compat",
    "content": "Image Compatibility commands. validate-node . Perform node validation based on its associated image compatibility artifact. –image . The --image flag specifies the URL of the image containing compatibility metadata. –plain-http . The --plain-http flag forces the use of HTTP protocol for all registry communications. Default: false . –platform . The --platform flag specifies the artifact platform in the format os[/arch][/variant][:os_version]. –tags . The --tags flag specifies a list of tags that must match the tags set on the compatibility objects. –output-json . The --output-json flag prints the output as a JSON object. –registry-username . The --registry-username flag specifies the username for the registry. –registry-password-stdin . The --registry-password-stdin flag enables reading of registry password from stdin. –registry-token-stdin . The --registry-token-stdin flag enables reading of registry token from stdin. ",
    "url": "/node-feature-discovery/master/reference/node-feature-client-reference.html#compat",
    
    "relUrl": "/reference/node-feature-client-reference.html#compat"
  },"117": {
    "doc": "Node Feature client cmdline reference",
    "title": "Node Feature client cmdline reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/node-feature-client-reference.html",
    
    "relUrl": "/reference/node-feature-client-reference.html"
  },"118": {
    "doc": "NFD Operator",
    "title": "Deployment with NFD Operator",
    "content": " ",
    "url": "/node-feature-discovery/master/deployment/operator.html#deployment-with-nfd-operator",
    
    "relUrl": "/deployment/operator.html#deployment-with-nfd-operator"
  },"119": {
    "doc": "NFD Operator",
    "title": "Table of contents",
    "content": ". | Deployment | Uninstallation | . The Node Feature Discovery Operator automates installation, configuration and updates of NFD using a specific NodeFeatureDiscovery custom resource. This also provides good support for managing NFD as a dependency of other operators. ",
    "url": "/node-feature-discovery/master/deployment/operator.html#table-of-contents",
    
    "relUrl": "/deployment/operator.html#table-of-contents"
  },"120": {
    "doc": "NFD Operator",
    "title": "Deployment",
    "content": "Deployment using the Node Feature Discovery Operator is recommended to be done via operatorhub.io. | You need to have OLM installed. If you don’t, take a look at the latest release for detailed instructions. | Install the operator: . kubectl create -f https://operatorhub.io/install/nfd-operator.yaml . | Create NodeFeatureDiscovery object (in nfd namespace here): . cat &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: nfd --- apiVersion: nfd.kubernetes.io/v1 kind: NodeFeatureDiscovery metadata: name: my-nfd-deployment namespace: nfd spec: operand: image: gcr.io/k8s-staging-nfd/node-feature-discovery:master imagePullPolicy: IfNotPresent EOF . | . ",
    "url": "/node-feature-discovery/master/deployment/operator.html#deployment",
    
    "relUrl": "/deployment/operator.html#deployment"
  },"121": {
    "doc": "NFD Operator",
    "title": "Uninstallation",
    "content": "If you followed the deployment instructions above you can uninstall NFD with: . kubectl -n nfd delete NodeFeatureDiscovery my-nfd-deployment . Optionally, you can also remove the namespace: . kubectl delete ns nfd . See the node-feature-discovery-operator and OLM project documentation for instructions for uninstalling the operator and operator lifecycle manager, respectively. ",
    "url": "/node-feature-discovery/master/deployment/operator.html#uninstallation",
    
    "relUrl": "/deployment/operator.html#uninstallation"
  },"122": {
    "doc": "NFD Operator",
    "title": "NFD Operator",
    "content": " ",
    "url": "/node-feature-discovery/master/deployment/operator.html",
    
    "relUrl": "/deployment/operator.html"
  },"123": {
    "doc": "Kubectl plugin cmdline reference",
    "title": "Commandline flags of kubectl-nfd (plugin)",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/plugin-commandline-reference.html#commandline-flags-of-kubectl-nfd-plugin",
    
    "relUrl": "/reference/plugin-commandline-reference.html#commandline-flags-of-kubectl-nfd-plugin"
  },"124": {
    "doc": "Kubectl plugin cmdline reference",
    "title": "Table of contents",
    "content": ". | -h, -help | Validate . | -f / –nodefeature-file | . | Test . | -k, –kubeconfig | -s, –namespace | -n, –nodename | -f, –nodefeaturerule-file | . | DryRun . | -f, –nodefeaturerule-file | -n, –nodefeature-file | . | . To quickly view available command line flags execute kubectl nfd -help. -h, -help . Print usage and exit. ",
    "url": "/node-feature-discovery/master/reference/plugin-commandline-reference.html#table-of-contents",
    
    "relUrl": "/reference/plugin-commandline-reference.html#table-of-contents"
  },"125": {
    "doc": "Kubectl plugin cmdline reference",
    "title": "Validate",
    "content": "Validate a NodeFeatureRule file. -f / –nodefeature-file . The --nodefeature-file flag specifies the path to the NodeFeatureRule file to validate. ",
    "url": "/node-feature-discovery/master/reference/plugin-commandline-reference.html#validate",
    
    "relUrl": "/reference/plugin-commandline-reference.html#validate"
  },"126": {
    "doc": "Kubectl plugin cmdline reference",
    "title": "Test",
    "content": "Test a NodeFeatureRule file against a node without applying it. -k, –kubeconfig . The --kubeconfig flag specifies the path to the kubeconfig file to use for CLI requests. -s, –namespace . The --namespace flag specifies the namespace to use for CLI requests. Default: default. -n, –nodename . The --nodename flag specifies the name of the node to test the NodeFeatureRule against. -f, –nodefeaturerule-file . The --nodefeaturerule-file flag specifies the path to the NodeFeatureRule file to test. ",
    "url": "/node-feature-discovery/master/reference/plugin-commandline-reference.html#test",
    
    "relUrl": "/reference/plugin-commandline-reference.html#test"
  },"127": {
    "doc": "Kubectl plugin cmdline reference",
    "title": "DryRun",
    "content": "Process a NodeFeatureRule file against a NodeFeature file. -f, –nodefeaturerule-file . The --nodefeaturerule-file flag specifies the path to the NodeFeatureRule file to test. -n, –nodefeature-file . The --nodefeature-file flag specifies the path to the NodeFeature file to test. ",
    "url": "/node-feature-discovery/master/reference/plugin-commandline-reference.html#dryrun",
    
    "relUrl": "/reference/plugin-commandline-reference.html#dryrun"
  },"128": {
    "doc": "Kubectl plugin cmdline reference",
    "title": "Kubectl plugin cmdline reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/plugin-commandline-reference.html",
    
    "relUrl": "/reference/plugin-commandline-reference.html"
  },"129": {
    "doc": "Quick start",
    "title": "Quick start",
    "content": "Minimal steps to deploy latest released version of NFD in your cluster. ",
    "url": "/node-feature-discovery/master/get-started/quick-start.html",
    
    "relUrl": "/get-started/quick-start.html"
  },"130": {
    "doc": "Quick start",
    "title": "Installation",
    "content": "Deploy with kustomize – creates a new namespace, service and required RBAC rules and deploys nfd-master and nfd-worker daemons. kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/default?ref=master\" . ",
    "url": "/node-feature-discovery/master/get-started/quick-start.html#installation",
    
    "relUrl": "/get-started/quick-start.html#installation"
  },"131": {
    "doc": "Quick start",
    "title": "Verify",
    "content": "Wait until NFD master and NFD worker are running. $ kubectl -n node-feature-discovery get ds,deploy NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/nfd-worker 2 2 2 2 2 &lt;none&gt; 10s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nfd-master 1/1 1 1 17s . Check that NFD feature labels have been created . $ kubectl get no -o json | jq \".items[].metadata.labels\" { \"kubernetes.io/arch\": \"amd64\", \"kubernetes.io/os\": \"linux\", \"feature.node.kubernetes.io/cpu-cpuid.ADX\": \"true\", \"feature.node.kubernetes.io/cpu-cpuid.AESNI\": \"true\", \"feature.node.kubernetes.io/cpu-cpuid.AVX\": \"true\", ... ",
    "url": "/node-feature-discovery/master/get-started/quick-start.html#verify",
    
    "relUrl": "/get-started/quick-start.html#verify"
  },"132": {
    "doc": "Quick start",
    "title": "Use node labels",
    "content": "Create a pod targeting a distinguishing feature (select a valid feature from the list printed on the previous step) . $ cat &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: feature-dependent-pod spec: containers: - image: registry.k8s.io/pause name: pause nodeSelector: # Select a valid feature feature.node.kubernetes.io/cpu-cpuid.AESNI: 'true' EOF pod/feature-dependent-pod created . See that the pod is running on a desired node . $ kubectl get po feature-dependent-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES feature-dependent-pod 1/1 Running 0 23s 10.36.0.4 node-2 &lt;none&gt; &lt;none&gt; . ",
    "url": "/node-feature-discovery/master/get-started/quick-start.html#use-node-labels",
    
    "relUrl": "/get-started/quick-start.html#use-node-labels"
  },"133": {
    "doc": "Quick start",
    "title": "Additional Optional Installation Steps",
    "content": "Deploy nfd-topology-updater . To deploy nfd-topology-updater use the topologyupdater kustomize overlay. kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/topologyupdater?ref=master\" . Verify nfd-topology-updater . Wait until nfd-topology-updater is running. $ kubectl -n node-feature-discovery get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/nfd-topology-updater 2 2 2 2 2 &lt;none&gt; 5s . Check that the NodeResourceTopology objects are created . $ kubectl get noderesourcetopologies.topology.node.k8s.io NAME AGE kind-control-plane 23s kind-worker 23s . ",
    "url": "/node-feature-discovery/master/get-started/quick-start.html#additional-optional-installation-steps",
    
    "relUrl": "/get-started/quick-start.html#additional-optional-installation-steps"
  },"134": {
    "doc": "Topology Updater Cmdline Reference",
    "title": "NFD-Topology-Updater Commandline Flags",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/topology-updater-commandline-reference.html#nfd-topology-updater-commandline-flags",
    
    "relUrl": "/reference/topology-updater-commandline-reference.html#nfd-topology-updater-commandline-flags"
  },"135": {
    "doc": "Topology Updater Cmdline Reference",
    "title": "Table of Contents",
    "content": ". | -h, -help | -version | -config | -no-publish | -oneshot | -port | -sleep-interval | -watch-namespace | -kubelet-config-uri | -api-auth-token-file | -podresources-socket | -pods-fingerprint | -kubelet-state-dir | . To quickly view available command line flags execute nfd-topology-updater -help. In a docker container: . docker run gcr.io/k8s-staging-nfd/node-feature-discovery:master \\ nfd-topology-updater -help . -h, -help . Print usage and exit. -version . Print version and exit. -config . The -config flag specifies the path of the nfd-topology-updater configuration file to use. Default: /etc/kubernetes/node-feature-discovery/nfd-topology-updater.conf . Example: . nfd-topology-updater -config=/opt/nfd/nfd-topology-updater.conf . -no-publish . The -no-publish flag makes for a “dry-run” flag for nfd-topology-updater. NFD-Topology-Updater runs resource hardware topology detection normally, but NodeResourceTopology objects are not created or updated. Default: false . Example: . nfd-topology-updater -no-publish . -oneshot . The -oneshot flag causes nfd-topology-updater to exit after one pass of resource hardware topology detection. Default: false . Example: . nfd-topology-updater -oneshot -no-publish . -port . The -port flag specifies the port on which metrics and healthz endpoints are served on. Default: 8080 . Example: . nfd-topology-updater -port=12345 . -sleep-interval . The -sleep-interval specifies the interval between resource hardware topology re-examination (and CR updates). zero means no CR updates on interval basis. Default: 60s . Example: . nfd-topology-updater -sleep-interval=1h . -watch-namespace . The -watch-namespace specifies the namespace to ensure that resource hardware topology examination only happens for the pods running in the specified namespace. Pods that are not running in the specified namespace are not considered during resource accounting. This is particularly useful for testing/debugging purpose. A “*” value would mean that all the pods would be considered during the accounting process. Default: “*” . Example: . nfd-topology-updater -watch-namespace=rte . -kubelet-config-uri . The -kubelet-config-uri specifies the path to the Kubelet’s configuration. Note that the URi could either be a local host file or an HTTP endpoint. Default: https://${NODE_ADDRESS}:10250/configz . Example: . nfd-topology-updater -kubelet-config-uri=file:///var/lib/kubelet/config.yaml . -api-auth-token-file . The -api-auth-token-file specifies the path to the api auth token file which is used to retrieve Kubelet’s configuration from Kubelet secure port, only taking effect when -kubelet-config-uri is https. Note that this token file must bind to a role that has the get capability to nodes/proxy resources. Default: /var/run/secrets/kubernetes.io/serviceaccount/token . Example: . nfd-topology-updater -token-file=/var/run/secrets/kubernetes.io/serviceaccount/token . -podresources-socket . The -podresources-socket specifies the path to the Unix socket where kubelet exports a gRPC service to enable discovery of in-use CPUs and devices, and to provide metadata for them. Default: /host-var/lib/kubelet/pod-resources/kubelet.sock . Example: . nfd-topology-updater -podresources-socket=/var/lib/kubelet/pod-resources/kubelet.sock . -pods-fingerprint . Enables compute and report the pod set fingerprint in the NRT. A pod fingerprint is a compact representation of the “node state” regarding resources. Default: true . Example: . nfd-topology-updater -pods-fingerprint=false . -kubelet-state-dir . The -kubelet-state-dir specifies the path to the Kubelet state directory, where state and checkpoint files are stored. The files are mount as read-only and cannot be change by the updater. Enabled by default. Passing an empty string will disable the watching. Default: /host-var/lib/kubelet . Example: . nfd-topology-updater -kubelet-state-dir=/var/lib/kubelet . ",
    "url": "/node-feature-discovery/master/reference/topology-updater-commandline-reference.html#table-of-contents",
    
    "relUrl": "/reference/topology-updater-commandline-reference.html#table-of-contents"
  },"136": {
    "doc": "Topology Updater Cmdline Reference",
    "title": "Topology Updater Cmdline Reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/topology-updater-commandline-reference.html",
    
    "relUrl": "/reference/topology-updater-commandline-reference.html"
  },"137": {
    "doc": "Topology-Updater config reference",
    "title": "Configuration file reference of nfd-topology-updater",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/topology-updater-configuration-reference.html#configuration-file-reference-of-nfd-topology-updater",
    
    "relUrl": "/reference/topology-updater-configuration-reference.html#configuration-file-reference-of-nfd-topology-updater"
  },"138": {
    "doc": "Topology-Updater config reference",
    "title": "Table of contents",
    "content": ". | excludeList . | excludeList.* | . | . See the sample configuration file for a full example configuration. ",
    "url": "/node-feature-discovery/master/reference/topology-updater-configuration-reference.html#table-of-contents",
    
    "relUrl": "/reference/topology-updater-configuration-reference.html#table-of-contents"
  },"139": {
    "doc": "Topology-Updater config reference",
    "title": "excludeList",
    "content": "The excludeList specifies a key-value map of allocated resources that should not be examined by the topology-updater agent per node. Each key is a node name with a value as a list of resources that should not be examined by the agent for that specific node. Default: empty . Example: . excludeList: nodeA: [hugepages-2Mi] nodeB: [memory] nodeC: [cpu, hugepages-2Mi] . excludeList.* . excludeList.* is a special value that use to specify all nodes. A resource that would be listed under this key, would be excluded from all nodes. Default: empty . Example: . excludeList: '*': [hugepages-2Mi] . ",
    "url": "/node-feature-discovery/master/reference/topology-updater-configuration-reference.html#excludelist",
    
    "relUrl": "/reference/topology-updater-configuration-reference.html#excludelist"
  },"140": {
    "doc": "Topology-Updater config reference",
    "title": "Topology-Updater config reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/topology-updater-configuration-reference.html",
    
    "relUrl": "/reference/topology-updater-configuration-reference.html"
  },"141": {
    "doc": "Uninstallation",
    "title": "Uninstallation",
    "content": ". Follow the uninstallation instructions of the deployment method used (kustomize, helm or operator). ",
    "url": "/node-feature-discovery/master/deployment/uninstallation.html",
    
    "relUrl": "/deployment/uninstallation.html"
  },"142": {
    "doc": "Uninstallation",
    "title": "Removing feature labels",
    "content": "NOTE: This is unnecessary when using the Helm chart for deployment as it will clean up the nodes when NFD is uninstalled. NFD-Master has a special -prune command line flag for removing all nfd-related node labels, annotations, extended resources and taints from the cluster. kubectl apply -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/prune?ref=master\" kubectl -n node-feature-discovery wait job.batch/nfd-master --for=condition=complete &amp;&amp; \\ kubectl delete -k \"https://github.com/kubernetes-sigs/node-feature-discovery/deployment/overlays/prune?ref=master\" . NOTE: You must run prune before removing the RBAC rules (serviceaccount, clusterrole and clusterrolebinding). ",
    "url": "/node-feature-discovery/master/deployment/uninstallation.html#removing-feature-labels",
    
    "relUrl": "/deployment/uninstallation.html#removing-feature-labels"
  },"143": {
    "doc": "Using node labels",
    "title": "Using node labels",
    "content": ". Nodes with specific features can be targeted using the nodeSelector field. The following example shows how to target nodes with Intel TurboBoost enabled. apiVersion: v1 kind: Pod metadata: labels: env: test name: golang-test spec: containers: - image: golang name: go1 nodeSelector: feature.node.kubernetes.io/cpu-pstate.turbo: 'true' . For more details on targeting nodes, see node selection. ",
    "url": "/node-feature-discovery/master/usage/using-labels.html",
    
    "relUrl": "/usage/using-labels.html"
  },"144": {
    "doc": "Versions",
    "title": "Versions and deprecation",
    "content": ". ",
    "url": "/node-feature-discovery/master/reference/versions.html#versions-and-deprecation",
    
    "relUrl": "/reference/versions.html#versions-and-deprecation"
  },"145": {
    "doc": "Versions",
    "title": "Supported versions",
    "content": "Node Feature Discovery follows semantic versioning where the version number consists of three components, i.e. MAJOR.MINOR.PATCH. The most recent two minor releases (or release branches) of Node Feature Discovery are supported. That is, with X being the latest release, X and X-1 are supported and X-1 reaches end-of-life when X+1 is released. ",
    "url": "/node-feature-discovery/master/reference/versions.html#supported-versions",
    
    "relUrl": "/reference/versions.html#supported-versions"
  },"146": {
    "doc": "Versions",
    "title": "Deprecation policy",
    "content": "Feature labels . Built-in feature labels and features are supported for 2 releases after being deprecated, at minimum. That is, if a feature label is deprecated in version X, it will be supported in X+1 and X+2 and may be dropped in X+3. Configuration options . Command-line flags and configuration file options are supported for 1 more release after being deprecated, at minimum. That is, if option/flag is deprecated in version X, it will be supported in X+1 and may be removed in X+2. The same policy (support for 1 release after deprecation) also applies to Helm chart parameters. ",
    "url": "/node-feature-discovery/master/reference/versions.html#deprecation-policy",
    
    "relUrl": "/reference/versions.html#deprecation-policy"
  },"147": {
    "doc": "Versions",
    "title": "Kubernetes compatibility",
    "content": "Node Feature Discovery is compatible with Kubernetes v1.24 and later. ",
    "url": "/node-feature-discovery/master/reference/versions.html#kubernetes-compatibility",
    
    "relUrl": "/reference/versions.html#kubernetes-compatibility"
  },"148": {
    "doc": "Versions",
    "title": "Versions",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/versions.html",
    
    "relUrl": "/reference/versions.html"
  },"149": {
    "doc": "Worker cmdline reference",
    "title": "Commandline flags of nfd-worker",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/worker-commandline-reference.html#commandline-flags-of-nfd-worker",
    
    "relUrl": "/reference/worker-commandline-reference.html#commandline-flags-of-nfd-worker"
  },"150": {
    "doc": "Worker cmdline reference",
    "title": "Table of contents",
    "content": ". | -h, -help | -version | -feature-gates | -config | -options | -kubeconfig | -feature-sources | -label-sources | -port | -no-publish | -no-owner-refs | -oneshot | Logging | . To quickly view available command line flags execute nfd-worker -help. In a docker container: . docker run gcr.io/k8s-staging-nfd/node-feature-discovery:master nfd-worker -help . -h, -help . Print usage and exit. -version . Print version and exit. -feature-gates . The -feature-gates flag is used to enable or disable non GA features. The list of available feature gates can be found in the feature gates documentation. Example: . nfd-master -feature-gates NodeFeatureGroupAPI=true . -config . The -config flag specifies the path of the nfd-worker configuration file to use. Default: /etc/kubernetes/node-feature-discovery/nfd-worker.conf . Example: . nfd-worker -config=/opt/nfd/worker.conf . -options . The -options flag may be used to specify and override configuration file options directly from the command line. The required format is the same as in the config file i.e. JSON or YAML. Configuration options specified via this flag will override those from the configuration file: . Default: empty . Example: . nfd-worker -options='{\"sources\":{\"cpu\":{\"cpuid\":{\"attributeWhitelist\":[\"AVX\",\"AVX2\"]}}}}' . -kubeconfig . The -kubeconfig flag specifies the kubeconfig to use for connecting to the Kubernetes API server. It is needed for manipulating NodeFeature objects. An empty value (which is also the default) implies in-cluster kubeconfig. Default: empty . Example: . nfd-worker -kubeconfig ${HOME}/.kube/config . -feature-sources . The -feature-sources flag specifies a comma-separated list of enabled feature sources. A special value all enables all sources. Prefixing a source name with - indicates that the source will be disabled instead - this is only meaningful when used in conjunction with all. This command line flag allows completely disabling the feature detection so that neither standard feature labels are generated nor the raw feature data is available for custom rule processing. Consider using the core.featureSources config file option, instead, allowing dynamic configurability. NOTE: This flag takes precedence over the core.featureSources configuration file option. Default: all . Example: . nfd-worker -feature-sources=all,-pci . -label-sources . The -label-sources flag specifies a comma-separated list of enabled label sources. A special value all enables all sources. Prefixing a source name with - indicates that the source will be disabled instead - this is only meaningful when used in conjunction with all. Consider using the core.labelSources config file option, instead, allowing dynamic configurability. NOTE: This flag takes precedence over the core.labelSources configuration file option. Default: all . Example: . nfd-worker -label-sources=kernel,system,local . -port . The -port flag specifies the port on which metrics and healthz endpoints are served on. Default: 8080 . Example: . nfd-worker -port=12345 . -no-publish . The -no-publish flag disables all communication with the nfd-master and the Kubernetes API server. It is effectively a “dry-run” flag for nfd-worker. NFD-Worker runs feature detection normally, but no labeling requests are sent to nfd-master and no NodeFeature objects are created or updated in the API server. NOTE: This flag takes precedence over the core.noPublish configuration file option. Default: false . Example: . nfd-worker -no-publish . -no-owner-refs . The -no-owner-refs flag disables setting the owner references to Pod of the NodeFeature object. NOTE: This flag takes precedence over the core.noOwnerRefs configuration file option. Default: false . Example: . nfd-worker -no-owner-refs . -oneshot . The -oneshot flag causes nfd-worker to exit after one pass of feature detection. Default: false . Example: . nfd-worker -oneshot -no-publish . Logging . The following logging-related flags are inherited from the klog package. NOTE: The logger setup can also be specified via the core.klog configuration file options. However, the command line flags take precedence over any corresponding config file options specified. -add_dir_header . If true, adds the file directory to the header of the log messages. Default: false . -alsologtostderr . Log to standard error as well as files. Default: false . -log_backtrace_at . When logging hits line file:N, emit a stack trace. Default: empty . -log_dir . If non-empty, write log files in this directory. Default: empty . -log_file . If non-empty, use this log file. Default: empty . -log_file_max_size . Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. Default: 1800 . -logtostderr . Log to standard error instead of files . Default: true . -skip_headers . If true, avoid header prefixes in the log messages. Default: false . -skip_log_headers . If true, avoid headers when opening log files. Default: false . -stderrthreshold . Logs at or above this threshold go to stderr. Default: 2 . -v . Number for the log level verbosity. Default: 0 . -vmodule . Comma-separated list of pattern=N settings for file-filtered logging. Default: empty . ",
    "url": "/node-feature-discovery/master/reference/worker-commandline-reference.html#table-of-contents",
    
    "relUrl": "/reference/worker-commandline-reference.html#table-of-contents"
  },"151": {
    "doc": "Worker cmdline reference",
    "title": "Worker cmdline reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/worker-commandline-reference.html",
    
    "relUrl": "/reference/worker-commandline-reference.html"
  },"152": {
    "doc": "Worker config reference",
    "title": "Configuration file reference of nfd-worker",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/worker-configuration-reference.html#configuration-file-reference-of-nfd-worker",
    
    "relUrl": "/reference/worker-configuration-reference.html#configuration-file-reference-of-nfd-worker"
  },"153": {
    "doc": "Worker config reference",
    "title": "Table of contents",
    "content": ". | core . | core.sleepInterval | core.featureSources | core.labelSources | core.sources | core.labelWhiteList | core.noPublish | core.noOwnerRefs | core.klog | . | sources . | sources.cpu | sources.kernel | sources.local | sources.pci | sources.usb | sources.custom | . | . See the sample configuration file for a full example configuration. ",
    "url": "/node-feature-discovery/master/reference/worker-configuration-reference.html#table-of-contents",
    
    "relUrl": "/reference/worker-configuration-reference.html#table-of-contents"
  },"154": {
    "doc": "Worker config reference",
    "title": "core",
    "content": "The core section contains common configuration settings that are not specific to any particular feature source. core.sleepInterval . core.sleepInterval specifies the interval between consecutive passes of feature (re-)detection, and thus also the interval between node re-labeling. A non-positive value implies infinite sleep interval, i.e. no re-detection or re-labeling is done. Default: 60s . Example: . core: sleepInterval: 60s . core.featureSources . core.featureSources specifies the list of enabled feature sources. A special value all enables all sources. Prefixing a source name with - indicates that the source will be disabled instead - this is only meaningful when used in conjunction with all. This option allows completely disabling the feature detection so that neither standard feature labels are generated nor the raw feature data is available for custom rule processing. Default: [all] . Example: . core: # Enable all but cpu and local sources featureSources: - \"all\" - \"-cpu\" - \"-local\" . core: # Enable only cpu and local sources featureSources: - \"cpu\" - \"local\" . core.labelSources . core.labelSources specifies the list of enabled label sources. A special value all enables all sources. Prefixing a source name with - indicates that the source will be disabled instead - this is only meaningful when used in conjunction with all. This configuration option affects the generation of node labels but not the actual discovery of the underlying feature data that is used e.g. in custom/NodeFeatureRule rules. NOTE: Overridden by the -label-sources command line flag and the core.sources configurations option (if either of them is specified). Default: [all] . Example: . core: # Enable all but cpu and system sources labelSources: - \"all\" - \"-cpu\" - \"-system\" . core: # Enable only cpu and system sources labelSources: - \"cpu\" - \"system\" . core.sources . DEPRECATED: use core.labelSources instead. NOTE: core.sources takes precedence over the core.labelSources configuration file option. core.labelWhiteList . core.labelWhiteList specifies a regular expression for filtering feature labels based on the label name. Non-matching labels are not published. NOTE: The regular expression is only matches against the “basename” part of the label, i.e. to the part of the name after ‘/’. The label prefix (or namespace) is omitted. Default: null . Example: . core: labelWhiteList: '^cpu-cpuid' . core.noPublish . Setting core.noPublish to true disables all communication with the nfd-master and the Kubernetes API server. It is effectively a “dry-run” option. NFD-Worker runs feature detection normally, but no labeling requests are sent to nfd-master and no NodeFeature objects are created or updated in the API server. NOTE: Overridden by the -no-publish command line flag (if specified). Default: false . Example: . core: noPublish: true . core.noOwnerRefs . Setting core.noOwnerRefs to true disables setting the owner references of the NodeFeature object created by the nfd-worker. NOTE: Overridden by the -no-owner-refs command line flag (if specified). Default: false . Example: . core: noOwnerRefs: true . core.klog . The following options specify the logger configuration. NOTE: The logger options can also be specified via command line flags which take precedence over any corresponding config file options. core.klog.addDirHeader . If true, adds the file directory to the header of the log messages. Default: false . core.klog.alsologtostderr . Log to standard error as well as files. Default: false . core.klog.logBacktraceAt . When logging hits line file:N, emit a stack trace. Default: empty . core.klog.logDir . If non-empty, write log files in this directory. Default: empty . core.klog.logFile . If non-empty, use this log file. Default: empty . core.klog.logFileMaxSize . Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. Default: 1800 . core.klog.logtostderr . Log to standard error instead of files . Default: true . core.klog.skipHeaders . If true, avoid header prefixes in the log messages. Default: false . core.klog.skipLogHeaders . If true, avoid headers when opening log files. Default: false . core.klog.stderrthreshold . Logs at or above this threshold go to stderr (default 2) . core.klog.v . Number for the log level verbosity. Default: 0 . core.klog.vmodule . Comma-separated list of pattern=N settings for file-filtered logging. Default: empty . ",
    "url": "/node-feature-discovery/master/reference/worker-configuration-reference.html#core",
    
    "relUrl": "/reference/worker-configuration-reference.html#core"
  },"155": {
    "doc": "Worker config reference",
    "title": "sources",
    "content": "The sources section contains feature source specific configuration parameters. sources.cpu . sources.cpu.cpuid . sources.cpu.cpuid.attributeBlacklist . Prevent publishing cpuid features listed in this option. NOTE: overridden by sources.cpu.cpuid.attributeWhitelist (if specified) . Default: [AVX10, BMI1, BMI2, CLMUL, CMOV, CX16, ERMS, F16C, HTT, LZCNT, MMX, MMXEXT, NX, POPCNT, RDRAND, RDSEED, RDTSCP, SGX, SGXLC, SSE, SSE2, SSE3, SSE4.1, SSE4.2, SSSE3, TDX_GUEST] . Example: . sources: cpu: cpuid: attributeBlacklist: [MMX, MMXEXT] . sources.cpu.cpuid.attributeWhitelist . Only publish the cpuid features listed in this option. NOTE: takes precedence over sources.cpu.cpuid.attributeBlacklist . Default: empty . Example: . sources: cpu: cpuid: attributeWhitelist: [AVX512BW, AVX512CD, AVX512DQ, AVX512F, AVX512VL] . sources.kernel . sources.kernel.kconfigFile . Path of the kernel config file. If empty, NFD runs a search in the well-known standard locations. Default: empty . Example: . sources: kernel: kconfigFile: \"/path/to/kconfig\" . sources.kernel.configOpts . Kernel configuration options to publish as feature labels. Default: [NO_HZ, NO_HZ_IDLE, NO_HZ_FULL, PREEMPT] . Example: . sources: kernel: configOpts: [NO_HZ, X86, DMI] . sources.local . sources.pci . sources.pci.deviceClassWhitelist . List of PCI device class IDs for which to publish a label. Can be specified as a main class only (e.g. 03) or full class-subclass combination (e.g. 0300) - the former implies that all subclasses are accepted. The format of the labels can be further configured with deviceLabelFields. Default: [\"03\", \"0b40\", \"12\"] . Example: . sources: pci: deviceClassWhitelist: [\"0200\", \"03\"] . sources.pci.deviceLabelFields . The set of PCI ID fields to use when constructing the name of the feature label. Valid fields are class, vendor, device, subsystem_vendor and subsystem_device. Default: [class, vendor] . Example: . sources: pci: deviceLabelFields: [class, vendor, device] . With the example config above NFD would publish labels like: feature.node.kubernetes.io/pci-&lt;class-id&gt;_&lt;vendor-id&gt;_&lt;device-id&gt;.present=true . sources.usb . sources.usb.deviceClassWhitelist . List of USB device class IDs for which to publish a feature label. The format of the labels can be further configured with deviceLabelFields. Default: [\"0e\", \"ef\", \"fe\", \"ff\"] . Example: . sources: usb: deviceClassWhitelist: [\"ef\", \"ff\"] . sources.usb.deviceLabelFields . The set of USB ID fields from which to compose the name of the feature label. Valid fields are class, vendor, device and serial. Default: [class, vendor, device] . Example: . sources: pci: deviceLabelFields: [class, vendor] . With the example config above NFD would publish labels like: feature.node.kubernetes.io/usb-&lt;class-id&gt;_&lt;vendor-id&gt;.present=true . sources.custom . List of rules to process in the custom feature source to create user-specific labels. Refer to the documentation of the custom feature source for details of the available rules and their configuration. Default: empty . Example: . sources: custom: - name: \"my custom rule\" labels: my-custom-feature: \"true\" matchFeatures: - feature: kernel.loadedmodule matchExpressions: e1000e: {op: Exists} - feature: pci.device matchExpressions: class: {op: In, value: [\"0200\"]} vendor: {op: In, value: [\"8086\"]} . ",
    "url": "/node-feature-discovery/master/reference/worker-configuration-reference.html#sources",
    
    "relUrl": "/reference/worker-configuration-reference.html#sources"
  },"156": {
    "doc": "Worker config reference",
    "title": "Worker config reference",
    "content": " ",
    "url": "/node-feature-discovery/master/reference/worker-configuration-reference.html",
    
    "relUrl": "/reference/worker-configuration-reference.html"
  }
}
